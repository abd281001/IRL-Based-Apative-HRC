{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbM9yCSSBENR"
      },
      "source": [
        "# Dataset Generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTXojY_mZlY0",
        "outputId": "83d88ba8-feb1-45f9-cced-1f124ef6adfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature_map:  {'pot_A': 0, 'pot_B': 1, 'pot_C': 2, 'pot_D': 3, 'pan_A': 4, 'pan_B': 5, 'pan_C': 6, 'pan_D': 7, 'plate_A': 8, 'plate_B': 9, 'plate_C': 10, 'plate_D': 11, 'tomato_A': 12, 'tomato_B': 13, 'tomato_C': 14, 'tomato_D': 15, 'meat_A': 16, 'meat_B': 17, 'meat_C': 18, 'meat_D': 19, 'onion_A': 20, 'onion_B': 21, 'onion_C': 22, 'onion_D': 23, 'mushroom_A': 24, 'mushroom_B': 25, 'mushroom_C': 26, 'mushroom_D': 27, 'lettuce_A': 28, 'lettuce_B': 29, 'lettuce_C': 30, 'lettuce_D': 31, 'egg_A': 32, 'egg_B': 33, 'egg_C': 34, 'egg_D': 35, 'tomato_cut': 36, 'onion_cut': 37, 'mushroom_cut': 38, 'lettuce_cut': 39, 'stove_on': 40}\n",
            "Generating 1 state-aware demonstrations...\n",
            "[1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0]\n",
            "[0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0]\n",
            "[0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0]\n",
            "[0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1\n",
            " 0 0 0 0]\n",
            "[0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1\n",
            " 0 0 0 0]\n",
            "[0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1\n",
            " 0 0 0 0]\n",
            "[0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1\n",
            " 1 0 0 0]\n",
            "[0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1\n",
            " 1 0 0 0]\n",
            "[0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1\n",
            " 1 0 0 1]\n",
            "[0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1\n",
            " 1 0 0 1]\n",
            "[0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1\n",
            " 1 0 0 1]\n",
            "Done! Saved 1 trajectories to demonstrations_with_states.txt\n",
            "State Vector Size: 41\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "DATASET_SIZE = 1\n",
        "OUTPUT_FILE = \"demonstrations_with_states.txt\"\n",
        "\n",
        "# --- Feature Definitions ---\n",
        "# We track location (A,B,C,D) for these items:\n",
        "ITEMS = [\"pot\", \"pan\", \"plate\", \"tomato\", \"meat\", \"onion\", \"mushroom\", \"lettuce\", \"egg\"]\n",
        "# We track \"is_cut\" status for these items:\n",
        "CUTTABLES = [\"tomato\", \"onion\", \"mushroom\", \"lettuce\"]\n",
        "LOCATIONS = [\"A\", \"B\", \"C\", \"D\"]\n",
        "\n",
        "class StateTracker:\n",
        "    def __init__(self):\n",
        "        # Initialize feature map indices\n",
        "        self.feature_map = {}\n",
        "        idx = 0\n",
        "        self.num = 20\n",
        "\n",
        "        # 1. Location Features: e.g., \"pot_A\": 0, \"pot_B\": 1...\n",
        "        for item in ITEMS:\n",
        "            for loc in LOCATIONS:\n",
        "                self.feature_map[f\"{item}_{loc}\"] = idx\n",
        "                idx += 1\n",
        "\n",
        "        # 2. Cut Status Features: e.g., \"tomato_cut\": 36...\n",
        "        for item in CUTTABLES:\n",
        "            self.feature_map[f\"{item}_cut\"] = idx\n",
        "            idx += 1\n",
        "\n",
        "        # 3. Global Status\n",
        "        self.feature_map[\"stove_on\"] = idx\n",
        "        self.n_features = idx + 1\n",
        "        self.reset()\n",
        "\n",
        "\n",
        "        print(\"Feature_map: \", self.feature_map)\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Resets state to initial conditions (Everything at A, raw, stove off).\"\"\"\n",
        "        self.current_state = np.zeros(self.n_features, dtype=int)\n",
        "        # Set all items to be at Station A initially\n",
        "        for item in ITEMS: self.set_feature(f\"{item}_A\", 1)\n",
        "\n",
        "    def set_feature(self, key, value):\n",
        "        if key in self.feature_map: self.current_state[self.feature_map[key]] = value\n",
        "\n",
        "    def get_feature(self, key):\n",
        "        if key in self.feature_map: return self.current_state[self.feature_map[key]]\n",
        "        return 0\n",
        "\n",
        "    def get_state_vector(self):\n",
        "        if self.num:\n",
        "            print(self.current_state)\n",
        "            self.num -= 1\n",
        "        return self.current_state.copy()\n",
        "\n",
        "\n",
        "\n",
        "    def apply_action(self, action_str):\n",
        "        \"\"\"Parses action string and updates the internal state vector.\"\"\"\n",
        "        # Parse \"move (item origin to dest)\"\n",
        "        if action_str.startswith(\"move\"):\n",
        "            # extract content between parens: \"pot A to C\"\n",
        "            content = action_str[action_str.find(\"(\")+1 : action_str.find(\")\")]\n",
        "            parts = content.split()\n",
        "            item, origin, _, dest = parts[0], parts[1], parts[2], parts[3]\n",
        "\n",
        "            # Update State: Remove from origin, add to dest\n",
        "            self.set_feature(f\"{item}_{origin}\", 0)\n",
        "            self.set_feature(f\"{item}_{dest}\", 1)\n",
        "\n",
        "        # Parse \"cut (item loc)\"\n",
        "        elif action_str.startswith(\"cut\"):\n",
        "            content = action_str[action_str.find(\"(\")+1 : action_str.find(\")\")]\n",
        "            parts = content.split()\n",
        "            item = parts[0]\n",
        "            # Update State: Mark as cut\n",
        "            self.set_feature(f\"{item}_cut\", 1)\n",
        "        # Parse \"turn_on (stove C)\"\n",
        "        elif action_str.startswith(\"turn_on\"): self.set_feature(\"stove_on\", 1)\n",
        "        # Parse \"turn_off (stove C)\"\n",
        "        elif action_str.startswith(\"turn_off\"): self.set_feature(\"stove_on\", 0)\n",
        "\n",
        "class RecipeGenerator:\n",
        "    def __init__(self):\n",
        "        self.demos = []\n",
        "        self.tracker = StateTracker()\n",
        "\n",
        "    def _record_trajectory(self, actions):\n",
        "        \"\"\"Runs the actions through the state tracker and records (State, Action) pairs.\"\"\"\n",
        "        self.tracker.reset()\n",
        "        trajectory = []\n",
        "\n",
        "        for action in actions:\n",
        "            # 1. Capture State BEFORE action\n",
        "            state_vector = self.tracker.get_state_vector().tolist()\n",
        "            # 2. Record Pair\n",
        "            trajectory.append({\"state\": state_vector, \"action\": action})\n",
        "            # 3. Update State for next step\n",
        "            self.tracker.apply_action(action)\n",
        "\n",
        "        final_state_vector = self.tracker.get_state_vector().tolist()\n",
        "        trajectory.append({\"state\": final_state_vector, \"action\": \"stop\"})\n",
        "\n",
        "        self.demos.append(trajectory)\n",
        "\n",
        "    # --- Recipe Definitions ---\n",
        "    def generate_tomato_soup(self):         return [\"move (pot A to C)\", \"move (tomato A to B)\", \"cut (tomato B)\", \"move (tomato B to C)\", \"turn_on (stove C)\", \"move (plate A to D)\", \"move (pot C to D)\"]\n",
        "    def generate_grilled_steak(self):       return [\"move (pan A to C)\", \"move (meat A to C)\", \"turn_on (stove C)\", \"move (plate A to B)\", \"turn_off (stove C)\", \"move (meat C to B)\", \"move (plate B to D)\"]\n",
        "    def generate_mushroom_stew(self):       return [\"move (pot A to C)\", \"move (mushroom A to B)\", \"cut (mushroom B)\", \"move (mushroom B to C)\", \"move (onion A to B)\", \"cut (onion B)\", \"move (onion B to C)\", \"turn_on (stove C)\", \"move (plate A to D)\", \"move (pot C to D)\"]\n",
        "    def generate_salad(self):               return [\"move (lettuce A to B)\", \"cut (lettuce B)\", \"move (onion A to B)\", \"cut (onion B)\", \"move (plate A to B)\", \"move (plate B to D)\"]\n",
        "    def generate_burger(self):              return [\"move (pan A to C)\", \"move (meat A to C)\", \"turn_on (stove C)\", \"move (lettuce A to B)\", \"cut (lettuce B)\", \"turn_off (stove C)\", \"move (meat C to B)\", \"move (plate A to B)\", \"move (plate B to D)\"]\n",
        "    def generate_boiled_eggs(self):         return [\"move (pot A to C)\", \"move (egg A to C)\", \"turn_on (stove C)\", \"turn_off (stove C)\", \"move (plate A to C)\", \"move (plate C to D)\"]\n",
        "    def generate_tomato_onion_soup_1(self):         return [\"move (pot A to C)\", \"move (tomato A to B)\", \"cut (tomato B)\", \"move (tomato B to C)\", \"move (onion A to B)\", \"cut (onion B)\", \"move (onion B to C)\", \"turn_on (stove C)\", \"move (plate A to D)\", \"move (pot C to D)\"]\n",
        "    def generate_tomato_onion_soup_2(self):         return [\"move (pot A to C)\", \"move (onion A to B)\", \"cut (onion B)\", \"move (onion B to C)\", \"move (tomato A to B)\", \"cut (tomato B)\", \"move (tomato B to C)\", \"turn_on (stove C)\", \"move (plate A to D)\", \"move (pot C to D)\"]\n",
        "\n",
        "    def generate_random_dataset(self, count):\n",
        "        # available_recipes = [self.generate_tomato_soup, self.generate_grilled_steak, self.generate_mushroom_stew, self.generate_salad, self.generate_burger, self.generate_boiled_eggs]\n",
        "        available_recipes = [self.generate_tomato_onion_soup_1]\n",
        "        \n",
        "        print(f\"Generating {count} state-aware demonstrations...\")\n",
        "\n",
        "        for i in range(count):\n",
        "            recipe_func = random.choice(available_recipes)\n",
        "            actions = recipe_func()\n",
        "            self._record_trajectory(actions)\n",
        "\n",
        "    def save_to_file(self):\n",
        "        # We save as JSON Lines (each line is a full trajectory object)\n",
        "        # This is much safer for parsing lists of lists than raw text\n",
        "        with open(OUTPUT_FILE, \"w\") as f:\n",
        "            for demo in self.demos:\n",
        "                f.write(json.dumps(demo) + \"\\n\")\n",
        "        print(f\"Done! Saved {len(self.demos)} trajectories to {OUTPUT_FILE}\")\n",
        "        print(f\"State Vector Size: {self.tracker.n_features}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    gen = RecipeGenerator()\n",
        "    gen.generate_random_dataset(DATASET_SIZE)\n",
        "    gen.save_to_file()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeZ5ql8i7Zaj"
      },
      "source": [
        "# IRL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNjxi8mm7Mqz",
        "outputId": "75ca03c3-bdb4-46d8-ff7c-9b2501a6d163"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1 demonstrations\n",
            "Number of unique actions: 11\n",
            "Actions: ['cut (onion B)', 'cut (tomato B)', 'move (onion A to B)', 'move (onion B to C)', 'move (plate A to D)']...\n",
            "state_to_idx: {(0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1): 0, (0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1): 1, (0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0): 2, (0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1): 3, (0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0): 4, (0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0): 5, (0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0): 6, (0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0): 7, (0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0): 8, (0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0): 9, (1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0): 10}\n",
            "idx_to_state: {0: (0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1), 1: (0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1), 2: (0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0), 3: (0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1), 4: (0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0), 5: (0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0), 6: (0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0), 7: (0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0), 8: (0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0), 9: (0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0), 10: (1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0)}\n",
            "action_to_idx: {'cut (onion B)': 0, 'cut (tomato B)': 1, 'move (onion A to B)': 2, 'move (onion B to C)': 3, 'move (plate A to D)': 4, 'move (pot A to C)': 5, 'move (pot C to D)': 6, 'move (tomato A to B)': 7, 'move (tomato B to C)': 8, 'stop': 9, 'turn_on (stove C)': 10}\n",
            "idx_to_action: {0: 'cut (onion B)', 1: 'cut (tomato B)', 2: 'move (onion A to B)', 3: 'move (onion B to C)', 4: 'move (plate A to D)', 5: 'move (pot A to C)', 6: 'move (pot C to D)', 7: 'move (tomato A to B)', 8: 'move (tomato B to C)', 9: 'stop', 10: 'turn_on (stove C)'}\n",
            "Number of unique states: 11\n",
            "Feature matrix shape: (11, 18)\n",
            "\n",
            "Running Maximum Entropy IRL...\n",
            "\n",
            "Found 11 unique (state, action) pairs\n",
            "Empirical feature expectation norm: 61.4282\n",
            "Converged at iteration 1\n",
            "\n",
            "Final best gradient norm: 0.000000\n",
            "\n",
            "=== Results ===\n",
            "Learned reward weights shape: (18,)\n",
            "Reward weight statistics:\n",
            "  Mean: -0.0000\n",
            "  Std: 0.0000\n",
            "  Min: -0.0000\n",
            "  Max: 0.0000\n",
            "\n",
            "Recovered rewards statistics:\n",
            "  Mean: -0.0000\n",
            "  Std: 0.0000\n",
            "  Min: -0.0000\n",
            "  Max: -0.0000\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "\n",
        "def load_demonstrations(filepath):\n",
        "    \"\"\"\n",
        "    Load demonstrations from JSON file.\n",
        "    \n",
        "    Returns:\n",
        "        demonstrations: list of trajectories, each with (state, action) pairs\n",
        "        unique_actions: list of unique action strings\n",
        "    \"\"\"\n",
        "    with open(filepath, 'r') as f:\n",
        "        content = f.read()\n",
        "    \n",
        "    # Parse JSON arrays (one per line)\n",
        "    demo_lists = [json.loads(line) for line in content.strip().split('\\n')]\n",
        "    \n",
        "    demonstrations = []\n",
        "    all_actions = set()\n",
        "    \n",
        "    for demo in demo_lists:\n",
        "        trajectory = []\n",
        "        for step in demo:\n",
        "            state = tuple(step['state'])  # Convert to tuple for hashing\n",
        "            action = step['action']\n",
        "            trajectory.append((state, action))\n",
        "            all_actions.add(action)\n",
        "        demonstrations.append(trajectory)\n",
        "    \n",
        "    # Get state dimensionality from first state\n",
        "    unique_actions = sorted(list(all_actions))\n",
        "    \n",
        "    return demonstrations, unique_actions\n",
        "\n",
        "\n",
        "def create_state_action_mappings(demonstrations, unique_actions):\n",
        "    \"\"\"\n",
        "    Create mappings between states/actions and indices.\n",
        "    \n",
        "    Returns:\n",
        "        state_to_idx: dict mapping state tuples to integer indices\n",
        "        idx_to_state: dict mapping integer indices to state tuples\n",
        "        action_to_idx: dict mapping action strings to integer indices\n",
        "        idx_to_action: dict mapping integer indices to action strings\n",
        "    \"\"\"\n",
        "    # Collect all unique states\n",
        "    unique_states = set()\n",
        "    for trajectory in demonstrations:\n",
        "        for state, _ in trajectory:\n",
        "            unique_states.add(state)\n",
        "    \n",
        "    # Create state mappings\n",
        "    state_to_idx = {state: idx for idx, state in enumerate(sorted(unique_states))}\n",
        "    idx_to_state = {idx: state for state, idx in state_to_idx.items()}\n",
        "    \n",
        "    # Create action mappings\n",
        "    action_to_idx = {action: idx for idx, action in enumerate(unique_actions)}\n",
        "    idx_to_action = {idx: action for action, idx in action_to_idx.items()}\n",
        "    \n",
        "    print(\"state_to_idx:\", state_to_idx)\n",
        "    print(\"idx_to_state:\", idx_to_state)\n",
        "    print(\"action_to_idx:\", action_to_idx)\n",
        "    print(\"idx_to_action:\", idx_to_action)\n",
        "\n",
        "    return state_to_idx, idx_to_state, action_to_idx, idx_to_action\n",
        "\n",
        "\n",
        "# def create_feature_matrix(idx_to_state, state_dim):\n",
        "#     \"\"\"\n",
        "#     Create feature matrix from states.\n",
        "#     Uses the state vector directly as features.\n",
        "    \n",
        "#     Returns:\n",
        "#         feature_matrix: 2D array of shape (n_states, n_features)\n",
        "#     \"\"\"\n",
        "#     n_states = len(idx_to_state)\n",
        "    \n",
        "#     # Use state vector as features directly\n",
        "#     feature_matrix = np.zeros((n_states, state_dim))\n",
        "    \n",
        "#     for idx in range(n_states):\n",
        "#         state = idx_to_state[idx]\n",
        "#         feature_matrix[idx] = np.array(state)\n",
        "    \n",
        "#     print(\"feature_matrix:\", feature_matrix)\n",
        "#     np.savetxt(\"feature_matrix.txt\", feature_matrix, fmt=\"%.3f\")\n",
        "#     ## The feature matrix is just all the state vectors stacked up.\n",
        "#     return feature_matrix\n",
        "\n",
        "def create_feature_matrix(idx_to_state):\n",
        "    \"\"\"\n",
        "    Create task-discriminative features for kitchen environment.\n",
        "    State vector layout (41 dims):\n",
        "    - Indices 0-35: Object locations (9 objects × 4 locations)\n",
        "      - pot: 0-3, pan: 4-7, plate: 8-11\n",
        "      - tomato: 12-15, egg: 16-19, meat: 20-23\n",
        "      - lettuce: 24-27, onion: 28-31, potato: 32-35\n",
        "    - Indices 36-39: Cut status (tomato, lettuce, onion, potato)\n",
        "    - Index 40: Stove on/off\n",
        "    \"\"\"\n",
        "    n_states = len(idx_to_state)\n",
        "    features = []\n",
        "    \n",
        "    for idx in range(n_states):\n",
        "        state = np.array(idx_to_state[idx])\n",
        "        feat = []\n",
        "        \n",
        "        # Original location features (keep these)\n",
        "        for loc in range(4):\n",
        "            items_at_loc = 0\n",
        "            for item_type in range(9):\n",
        "                if state[item_type*4 + loc] == 1:\n",
        "                    items_at_loc += 1\n",
        "            feat.append(items_at_loc)\n",
        "        \n",
        "        feat.append(np.sum(state[36:40]))  # Feature 4: total cut items\n",
        "        feat.append(state[40])              # Feature 5: stove on\n",
        "        \n",
        "        # NEW DISCRIMINATIVE FEATURES\n",
        "        \n",
        "        # Feature 6: Pot at stove C (egg cooking setup)\n",
        "        pot_at_C = state[2]\n",
        "        feat.append(pot_at_C)\n",
        "        \n",
        "        # Feature 7: Pan at stove C (meat cooking setup)\n",
        "        pan_at_C = state[6]\n",
        "        feat.append(pan_at_C)\n",
        "        \n",
        "        # Feature 8: Egg at stove C (ready to cook)\n",
        "        egg_at_C = state[18]\n",
        "        feat.append(egg_at_C)\n",
        "        \n",
        "        # Feature 9: Tomato cut and at stove (tomato cooking)\n",
        "        tomato_cut = state[36]\n",
        "        tomato_at_C = state[14]\n",
        "        feat.append(tomato_cut * tomato_at_C * pot_at_C)\n",
        "        \n",
        "        # Feature 10: Meat at stove (meat cooking)\n",
        "        meat_at_C = state[22]\n",
        "        feat.append(meat_at_C * pan_at_C)\n",
        "        \n",
        "        # Feature 11: Lettuce cut (salad preparation)\n",
        "        lettuce_cut = state[37]\n",
        "        feat.append(lettuce_cut)\n",
        "        \n",
        "        # Feature 12: Onion cut (salad preparation)\n",
        "        onion_cut = state[38]\n",
        "        feat.append(onion_cut)\n",
        "        \n",
        "        # Feature 13: Vegetables at cutting board B\n",
        "        lettuce_at_B = state[25]\n",
        "        onion_at_B = state[29]\n",
        "        feat.append(lettuce_at_B + onion_at_B)\n",
        "        \n",
        "        # Feature 14: Tomato at cutting board (cooking prep)\n",
        "        tomato_at_B = state[13]\n",
        "        feat.append(tomato_at_B)\n",
        "        \n",
        "        # Feature 15: Stove active with cookable container\n",
        "        cooking_active = state[40] * (pot_at_C + pan_at_C)\n",
        "        feat.append(cooking_active)\n",
        "        \n",
        "        # Feature 16: Plate at serving location D\n",
        "        plate_at_D = state[11]\n",
        "        feat.append(plate_at_D)\n",
        "        \n",
        "        # Feature 17: Cooked food ready to serve\n",
        "        # (stove was on, now off, pot/pan still at C)\n",
        "        feat.append((1 - state[40]) * (pot_at_C + pan_at_C) * (egg_at_C + tomato_at_C + meat_at_C))\n",
        "        \n",
        "        features.append(feat)\n",
        "    \n",
        "    return np.array(features)\n",
        "# def create_feature_matrix(idx_to_state):\n",
        "#     \"\"\"\n",
        "#     Create more meaningful features instead of raw state vectors.\n",
        "#     \"\"\"\n",
        "#     n_states = len(idx_to_state)\n",
        "    \n",
        "#     # Design better features, e.g.:\n",
        "#     # - Number of items at each location\n",
        "#     # - Number of items cut\n",
        "#     # - Whether stove is on\n",
        "#     # - Items at correct goal locations\n",
        "#     features = []\n",
        "\n",
        "#     for idx in range(n_states):\n",
        "#         state = np.array(idx_to_state[idx])\n",
        "#         state_features = []\n",
        "        \n",
        "#         # Original 7 features\n",
        "#         for loc in range(4):\n",
        "#             items_at_loc = 0\n",
        "#             for item_type in range(9):\n",
        "#                 if state[item_type*4 + loc] == 1:\n",
        "#                     items_at_loc += 1\n",
        "#             state_features.append(items_at_loc)\n",
        "        \n",
        "#         state_features.append(np.sum(state[36:40]))  # num_cut\n",
        "#         state_features.append(state[40])              # stove_on\n",
        "        \n",
        "#         # Feature 8: Cut items at stove (ready to cook)\n",
        "#         cut_at_stove = 0\n",
        "#         if state[10] == 1:  # pot at C\n",
        "#             for cut_idx in range(36, 40):\n",
        "#                 cut_at_stove += state[cut_idx]\n",
        "#         state_features.append(cut_at_stove)\n",
        "        \n",
        "#         # Feature 9: Cookable container at stove\n",
        "#         cookable_at_C = state[2] + state[6]  # pot_C + pan_C\n",
        "#         state_features.append(cookable_at_C)\n",
        "        \n",
        "#         # Feature 10: Stove active WITH cookable\n",
        "#         productive_cooking = state[40] * cookable_at_C\n",
        "#         state_features.append(productive_cooking)\n",
        "        \n",
        "#         # Feature 11: Plate ready for serving\n",
        "#         plate_at_D = state[11]  # plate_D\n",
        "#         state_features.append(plate_at_D)\n",
        "        \n",
        "#         # Feature 12: Items at cutting board AND not yet cut\n",
        "#         items_at_B = sum([state[1], state[5], state[9]])  # pot, pan, plate at B\n",
        "#         uncut = 4 - np.sum(state[36:40])\n",
        "#         cutting_needed = items_at_B * (uncut > 0)\n",
        "#         state_features.append(cutting_needed)\n",
        "        \n",
        "#         features.append(state_features)\n",
        "    \n",
        "#     return np.array(features)  # Now (n_states, 13) features\n",
        "\n",
        "\n",
        "def max_entropy_inverse_rl_kitchen(demonstrations, state_to_idx, action_to_idx, feature_matrix, \n",
        "                                   temperature=2.0, gamma=0.9, n_iterations=100, learning_rate=0.05):\n",
        "    \"\"\"\n",
        "    Maximum Entropy Inverse RL adapted for kitchen task demonstrations.\n",
        "    Uses trajectory-based approach since we have sequential demonstrations.\n",
        "    \"\"\"\n",
        "    n_states = len(state_to_idx)\n",
        "    n_actions = len(action_to_idx)\n",
        "    n_features = feature_matrix.shape[1]\n",
        "    \n",
        "    # Initialize reward weights\n",
        "    reward_weights = np.zeros(n_features)\n",
        "    \n",
        "    # Compute empirical feature expectations from expert demonstrations\n",
        "    empirical_feature_expectations = np.zeros(n_features)\n",
        "    for trajectory in demonstrations:\n",
        "        # Compute discounted state visitation for this trajectory\n",
        "        discounted_visitation = np.zeros(n_states)\n",
        "        for t, (state, action) in enumerate(trajectory):\n",
        "            s_idx = state_to_idx[state]\n",
        "            discounted_visitation[s_idx] += (gamma ** t)\n",
        "        # Convert to feature expectations\n",
        "        traj_features = discounted_visitation @ feature_matrix\n",
        "        empirical_feature_expectations += traj_features\n",
        "    empirical_feature_expectations /= len(demonstrations)\n",
        "        \n",
        "    # Build transition model from demonstrations\n",
        "    # For each (s, a) pair, track which s' it leads to\n",
        "    transition_model = {}  # (s_idx, a_idx) -> s_next_idx\n",
        "    state_action_pairs = set()\n",
        "    \n",
        "    for trajectory in demonstrations:\n",
        "        for i in range(len(trajectory)):\n",
        "            state, action = trajectory[i]\n",
        "            s_idx = state_to_idx[state]\n",
        "            a_idx = action_to_idx[action]\n",
        "            state_action_pairs.add((s_idx, a_idx))\n",
        "            if i < len(trajectory) - 1:\n",
        "                next_state, _ = trajectory[i + 1]\n",
        "                s_next_idx = state_to_idx[next_state]\n",
        "                transition_model[(s_idx, a_idx)] = s_next_idx\n",
        "        #     # print(transition_model)\n",
        "        #     print(state_action_pairs)\n",
        "        # print(\"\\n\\n\")\n",
        "    \n",
        "    print(f\"\\nFound {len(state_action_pairs)} unique (state, action) pairs\")\n",
        "    print(f\"Empirical feature expectation norm: {np.linalg.norm(empirical_feature_expectations):.4f}\")\n",
        "    \n",
        "    # Gradient descent with adaptive learning rate\n",
        "    best_diff = float('inf')\n",
        "    patience = 0\n",
        "    best_weights = reward_weights.copy()\n",
        "    momentum = np.zeros(n_features)\n",
        "    momentum_beta = 0.9\n",
        "    \n",
        "    for iteration in range(n_iterations):\n",
        "        # Adaptive learning rate - decay if oscillating\n",
        "        current_lr = learning_rate * (0.95 ** (patience // 5))\n",
        "        # Compute rewards for each state\n",
        "        rewards = feature_matrix @ reward_weights\n",
        "        \n",
        "        # Compute state-action values using backward induction\n",
        "        # Since trajectories are sequential, we can compute exact values\n",
        "        q_values = {}  # (s_idx, a_idx) -> Q-value\n",
        "        values = np.zeros(n_states)\n",
        "        \n",
        "        # Initialize terminal state values\n",
        "        for s_idx in range(n_states): values[s_idx] = rewards[s_idx]\n",
        "        \n",
        "        # Backward value iteration\n",
        "        for vi_iter in range(30):\n",
        "            new_values = rewards.copy()\n",
        "            \n",
        "            for (s_idx, a_idx) in state_action_pairs:\n",
        "                if (s_idx, a_idx) in transition_model:\n",
        "                    s_next = transition_model[(s_idx, a_idx)]\n",
        "                    q_values[(s_idx, a_idx)] = rewards[s_idx] + gamma * values[s_next]\n",
        "                else: q_values[(s_idx, a_idx)] = rewards[s_idx]         # Terminal action\n",
        "            \n",
        "            # Update values using soft-max\n",
        "            for s_idx in range(n_states):\n",
        "                # Get all actions available from this state\n",
        "                available_qs = []\n",
        "                for a_idx in range(n_actions):\n",
        "                    if (s_idx, a_idx) in q_values: available_qs.append(q_values[(s_idx, a_idx)])\n",
        "                \n",
        "                if len(available_qs) > 0:\n",
        "                    available_qs = np.array(available_qs)\n",
        "                    max_q = np.max(available_qs)\n",
        "                    normalized_qs = (available_qs - max_q) / temperature\n",
        "                    new_values[s_idx] = max_q + temperature * np.log(np.sum(np.exp(normalized_qs)))\n",
        "            \n",
        "            if np.max(np.abs(new_values - values)) < 1e-6: break\n",
        "            values = new_values\n",
        "        \n",
        "        # Compute soft policy from Q-values\n",
        "        policy = {}\n",
        "        for s_idx in range(n_states):\n",
        "            # Get Q-values for all actions from this state\n",
        "            state_qs = []\n",
        "            state_actions = []\n",
        "            \n",
        "            for a_idx in range(n_actions):\n",
        "                if (s_idx, a_idx) in q_values:\n",
        "                    state_qs.append(q_values[(s_idx, a_idx)])\n",
        "                    state_actions.append(a_idx)\n",
        "\n",
        "            if len(state_qs) > 0:\n",
        "                state_qs = np.array(state_qs)\n",
        "                max_q = np.max(state_qs)\n",
        "                exp_qs = np.exp((state_qs - max_q) / temperature)\n",
        "                probs = exp_qs / np.sum(exp_qs)\n",
        "                \n",
        "                for a_idx, prob in zip(state_actions, probs): policy[(s_idx, a_idx)] = prob\n",
        "        \n",
        "        # Compute expected feature counts by rolling out policy\n",
        "        # Monte Carlo estimation: sample trajectories according to learned policy\n",
        "        expected_feature_counts = np.zeros(n_features)\n",
        "        n_samples = 100  # Number of trajectory samples\n",
        "        \n",
        "        for _ in range(n_samples):\n",
        "            # Sample a starting state from demonstrations\n",
        "            start_traj = demonstrations[np.random.randint(len(demonstrations))]\n",
        "            s_idx = state_to_idx[start_traj[0][0]]\n",
        "            \n",
        "            traj_features = np.zeros(n_features)\n",
        "            for t in range(20):  # Max trajectory length\n",
        "                # Add current state features\n",
        "                traj_features += (gamma ** t) * feature_matrix[s_idx]\n",
        "                \n",
        "                # Sample action according to policy\n",
        "                available_actions = []\n",
        "                action_probs = []\n",
        "                for a_idx in range(n_actions):\n",
        "                    if (s_idx, a_idx) in policy:\n",
        "                        available_actions.append(a_idx)\n",
        "                        action_probs.append(policy[(s_idx, a_idx)])\n",
        "                if len(available_actions) == 0: break\n",
        "                \n",
        "                action_probs = np.array(action_probs)\n",
        "                action_probs /= np.sum(action_probs)\n",
        "                a_idx = np.random.choice(available_actions, p=action_probs)\n",
        "                \n",
        "                # Transition to next state\n",
        "                if (s_idx, a_idx) in transition_model: s_idx = transition_model[(s_idx, a_idx)]\n",
        "                else: break\n",
        "            expected_feature_counts += traj_features\n",
        "        expected_feature_counts /= n_samples\n",
        "        \n",
        "        # Compute gradient\n",
        "        gradient = empirical_feature_expectations - expected_feature_counts\n",
        "        grad_norm = np.linalg.norm(gradient)\n",
        "        # Momentum update\n",
        "        momentum = momentum_beta * momentum + (1 - momentum_beta) * gradient\n",
        "        # Update weights with momentum\n",
        "        reward_weights += current_lr * momentum\n",
        "        # Track best solution\n",
        "        if grad_norm < best_diff:\n",
        "            best_diff = grad_norm\n",
        "            best_weights = reward_weights.copy()\n",
        "            patience = 0\n",
        "        else: patience += 1\n",
        "        # Reset to best if diverging too much\n",
        "        if patience > 15:\n",
        "            reward_weights = best_weights.copy()\n",
        "            momentum = np.zeros(n_features)\n",
        "            patience = 0\n",
        "            current_lr *= 0.5\n",
        "            if iteration > 0: print(f\"  Reset to best solution, reducing lr to {current_lr:.4f}\")\n",
        "        \n",
        "        if (iteration + 1) % 10 == 0: print(f\"Iteration {iteration + 1}: Gradient norm = {grad_norm:.6f}, \"\n",
        "                                            f\"Reward range = [{np.min(rewards):.2f}, {np.max(rewards):.2f}], \"\n",
        "                                            f\"Best = {best_diff:.6f}, LR = {current_lr:.4f}\")\n",
        "        # Early stopping\n",
        "        if grad_norm < 0.1:\n",
        "            print(f\"Converged at iteration {iteration + 1}\")\n",
        "            break\n",
        "    # Use best weights found\n",
        "    reward_weights = best_weights\n",
        "    recovered_rewards = feature_matrix @ reward_weights\n",
        "    print(f\"\\nFinal best gradient norm: {best_diff:.6f}\")\n",
        "    return reward_weights, recovered_rewards\n",
        "\n",
        "\n",
        "def predict_action(current_state, reward_weights, feature_matrix, state_to_idx, \n",
        "                   action_to_idx, idx_to_action, transition_model, \n",
        "                   temperature=2.0, gamma=0.9):\n",
        "    \"\"\"\n",
        "    Predict the next action given current state using learned reward function.\n",
        "    \n",
        "    Args:\n",
        "        current_state: tuple representing the current state\n",
        "        reward_weights: learned reward weights from IRL\n",
        "        feature_matrix: feature matrix for all states\n",
        "        state_to_idx: mapping from state to index\n",
        "        action_to_idx: mapping from action string to index\n",
        "        idx_to_action: mapping from action index to string\n",
        "        transition_model: dictionary (s_idx, a_idx) -> s_next_idx\n",
        "        temperature: softmax temperature for policy\n",
        "        gamma: discount factor\n",
        "        \n",
        "    Returns:\n",
        "        predicted_action: string of predicted action\n",
        "        action_probs: dict of {action_string: probability} for all valid actions\n",
        "    \"\"\"\n",
        "    # Check if state is known\n",
        "    if current_state not in state_to_idx:\n",
        "        print(f\"Warning: Unknown state! Not seen in demonstrations.\")\n",
        "        return None, {}\n",
        "    \n",
        "    s_idx = state_to_idx[current_state]\n",
        "    # Compute rewards\n",
        "    rewards = feature_matrix @ reward_weights\n",
        "    # Find all valid actions from this state\n",
        "    valid_actions = []\n",
        "    q_values_list = []\n",
        "    \n",
        "    for a_idx in range(len(action_to_idx)):\n",
        "        if (s_idx, a_idx) in transition_model:\n",
        "            s_next_idx = transition_model[(s_idx, a_idx)]\n",
        "            # Compute Q-value: Q(s,a) = r(s) + γ * V(s')\n",
        "            # Approximate V(s') ≈ r(s') for simplicity\n",
        "            q_value = rewards[s_idx] + gamma * rewards[s_next_idx]\n",
        "            \n",
        "            valid_actions.append(a_idx)\n",
        "            q_values_list.append(q_value)\n",
        "    if len(valid_actions) == 0:\n",
        "        print(f\"Warning: No valid actions from this state!\")\n",
        "        return None, {}\n",
        "    # Convert to numpy array\n",
        "    q_values_array = np.array(q_values_list)\n",
        "    # Compute softmax policy: π(a|s) ∝ exp(Q(s,a)/temperature)\n",
        "    max_q = np.max(q_values_array)\n",
        "    exp_q = np.exp((q_values_array - max_q) / temperature)\n",
        "    action_probs_array = exp_q / np.sum(exp_q)\n",
        "    # Create action probability dictionary\n",
        "    action_probs = {}\n",
        "    for a_idx, prob in zip(valid_actions, action_probs_array):\n",
        "        action_str = idx_to_action[a_idx]\n",
        "        action_probs[action_str] = prob\n",
        "    # Get most likely action\n",
        "    best_action_idx = valid_actions[np.argmax(action_probs_array)]\n",
        "    predicted_action = idx_to_action[best_action_idx]\n",
        "    \n",
        "    return predicted_action, action_probs\n",
        "\n",
        "\n",
        "def evaluate_policy(demonstrations, reward_weights, feature_matrix, \n",
        "                   state_to_idx, action_to_idx, idx_to_action, \n",
        "                   transition_model, temperature=2.0, gamma=0.9):\n",
        "    \"\"\"\n",
        "    Evaluate how well the learned policy matches expert demonstrations.\n",
        "    \n",
        "    Returns:\n",
        "        accuracy: fraction of actions correctly predicted\n",
        "        confusion_matrix: dict showing prediction results\n",
        "    \"\"\"\n",
        "    total_predictions = 0\n",
        "    correct_predictions = 0\n",
        "    # Track predictions for each action\n",
        "    action_results = {}  # action -> {\"correct\": count, \"total\": count}\n",
        "    for trajectory in demonstrations:\n",
        "        for i in range(len(trajectory) - 1):  # Exclude last state (no next action)\n",
        "            current_state, expert_action = trajectory[i]\n",
        "            # Predict action\n",
        "            predicted_action, action_probs = predict_action(\n",
        "                current_state, reward_weights, feature_matrix,\n",
        "                state_to_idx, action_to_idx, idx_to_action,\n",
        "                transition_model, temperature, gamma)\n",
        "            \n",
        "            if predicted_action is not None:\n",
        "                total_predictions += 1\n",
        "                # Check if prediction matches expert\n",
        "                if predicted_action == expert_action:\n",
        "                    correct_predictions += 1\n",
        "                    is_correct = True\n",
        "                else: is_correct = False\n",
        "                # Track per-action statistics\n",
        "                if expert_action not in action_results: action_results[expert_action] = {\"correct\": 0, \"total\": 0}\n",
        "                action_results[expert_action][\"total\"] += 1\n",
        "                if is_correct: action_results[expert_action][\"correct\"] += 1\n",
        "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
        "    return accuracy, action_results\n",
        "\n",
        "\n",
        "def test_predictions(demonstrations, reward_weights, feature_matrix,\n",
        "                    state_to_idx, idx_to_state, action_to_idx, idx_to_action,\n",
        "                    transition_model, n_examples=5, temperature=2.0, gamma=0.9):\n",
        "    \"\"\"\n",
        "    Show example predictions vs expert actions.\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Example Predictions ===\")\n",
        "    examples_shown = 0\n",
        "    for traj_idx, trajectory in enumerate(demonstrations):\n",
        "        if examples_shown >= n_examples: break\n",
        "        # Pick a random step in the middle of trajectory\n",
        "        if len(trajectory) > 2:\n",
        "            step_idx = len(trajectory) // 2\n",
        "            current_state, expert_action = trajectory[step_idx]\n",
        "            print(f\"\\nTrajectory {traj_idx + 1}, Step {step_idx + 1}:\")\n",
        "            print(f\"  Expert action: {expert_action}\")\n",
        "            # Get prediction\n",
        "            predicted_action, action_probs = predict_action(\n",
        "                current_state, reward_weights, feature_matrix,\n",
        "                state_to_idx, action_to_idx, idx_to_action,\n",
        "                transition_model, temperature, gamma)\n",
        "            print(f\"  Predicted action: {predicted_action}\")\n",
        "            print(f\"  Match: {'✓' if predicted_action == expert_action else '✗'}\")\n",
        "            # Show top 3 actions by probability\n",
        "            sorted_actions = sorted(action_probs.items(), key=lambda x: x[1], reverse=True)\n",
        "            print(f\"  Top 3 action probabilities:\")\n",
        "            for action, prob in sorted_actions[:3]:\n",
        "                marker = \"←\" if action == expert_action else \" \"\n",
        "                print(f\"    {action}: {prob:.4f} {marker}\")\n",
        "            examples_shown += 1\n",
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Load demonstrations\n",
        "    demonstrations, unique_actions = load_demonstrations('demonstrations_with_states.txt')\n",
        "    \n",
        "    print(f\"Loaded {len(demonstrations)} demonstrations\")\n",
        "    print(f\"Number of unique actions: {len(unique_actions)}\")\n",
        "    print(f\"Actions: {unique_actions[:5]}...\")  # Show first 5\n",
        "    \n",
        "    # Create mappings\n",
        "    state_to_idx, idx_to_state, action_to_idx, idx_to_action = \\\n",
        "        create_state_action_mappings(demonstrations, unique_actions)\n",
        "    \n",
        "    print(f\"Number of unique states: {len(state_to_idx)}\")\n",
        "\n",
        "    \n",
        "    # Build transition model for predictions\n",
        "    transition_model = {}\n",
        "    for trajectory in demonstrations:\n",
        "        for i in range(len(trajectory)):\n",
        "            state, action = trajectory[i]\n",
        "            s_idx = state_to_idx[state]\n",
        "            a_idx = action_to_idx[action]\n",
        "            \n",
        "            if i < len(trajectory) - 1:\n",
        "                next_state, _ = trajectory[i + 1]\n",
        "                s_next_idx = state_to_idx[next_state]\n",
        "                transition_model[(s_idx, a_idx)] = s_next_idx\n",
        "    \n",
        "    # Create feature matrix\n",
        "    feature_matrix = create_feature_matrix(idx_to_state)\n",
        "    \n",
        "    print(f\"Feature matrix shape: {feature_matrix.shape}\")\n",
        "    \n",
        "    # Run IRL\n",
        "    print(\"\\nRunning Maximum Entropy IRL...\")\n",
        "    reward_weights, recovered_rewards = max_entropy_inverse_rl_kitchen(\n",
        "        demonstrations=demonstrations,\n",
        "        state_to_idx=state_to_idx,\n",
        "        action_to_idx=action_to_idx,\n",
        "        feature_matrix=feature_matrix,\n",
        "        temperature=1.0,\n",
        "        gamma=0.95,\n",
        "        n_iterations=5,\n",
        "        learning_rate=0.01\n",
        "    )\n",
        "    \n",
        "    print(\"\\n=== Results ===\")\n",
        "    print(f\"Learned reward weights shape: {reward_weights.shape}\")\n",
        "    print(f\"Reward weight statistics:\")\n",
        "    print(f\"  Mean: {np.mean(reward_weights):.4f}\")\n",
        "    print(f\"  Std: {np.std(reward_weights):.4f}\")\n",
        "    print(f\"  Min: {np.min(reward_weights):.4f}\")\n",
        "    print(f\"  Max: {np.max(reward_weights):.4f}\")\n",
        "    \n",
        "    print(f\"\\nRecovered rewards statistics:\")\n",
        "    print(f\"  Mean: {np.mean(recovered_rewards):.4f}\")\n",
        "    print(f\"  Std: {np.std(recovered_rewards):.4f}\")\n",
        "    print(f\"  Min: {np.min(recovered_rewards):.4f}\")\n",
        "    print(f\"  Max: {np.max(recovered_rewards):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'cut (onion B)': 0, 'cut (tomato B)': 1, 'move (onion A to B)': 2, 'move (onion B to C)': 3, 'move (plate A to D)': 4, 'move (pot A to C)': 5, 'move (pot C to D)': 6, 'move (tomato A to B)': 7, 'move (tomato B to C)': 8, 'stop': 9, 'turn_on (stove C)': 10}\n"
          ]
        }
      ],
      "source": [
        "print(action_to_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Actual: move (pot A to C) \n",
            " Predicted: move (pot A to C) \n",
            "\n",
            "Actual: move (tomato A to B) \n",
            " Predicted: move (tomato A to B) \n",
            "\n",
            "Actual: cut (tomato B) \n",
            " Predicted: cut (tomato B) \n",
            "\n",
            "Actual: move (tomato B to C) \n",
            " Predicted: move (tomato B to C) \n",
            "\n",
            "Actual: move (onion A to B) \n",
            " Predicted: move (onion A to B) \n",
            "\n",
            "Actual: cut (onion B) \n",
            " Predicted: cut (onion B) \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example 1: Test the 5th step of trajectory 2\n",
        "# state = demonstrations[2][4][0]\n",
        "# action, probs = predict_action(\n",
        "#     state, reward_weights, feature_matrix,\n",
        "#     state_to_idx, action_to_idx, idx_to_action,\n",
        "#     transition_model\n",
        "# )\n",
        "# print(f\"Predicted: {action}\")\n",
        "# print(f\"Probabilities: {probs}\", \"\\n\\n\")\n",
        "\n",
        "# Example 2: Test the first step (initial state)\n",
        "for i in range (1):\n",
        "    for j in range(0,6):\n",
        "        state = demonstrations[i][j][0]\n",
        "        action, probs = predict_action(\n",
        "            state, reward_weights, feature_matrix,\n",
        "            state_to_idx, action_to_idx, idx_to_action,\n",
        "            transition_model\n",
        "        )\n",
        "        print(f\"Actual: {demonstrations[i][j][1]} \\n Predicted: {action} \\n\")\n",
        "    print(\"\\n\\n\\n\")\n",
        "\n",
        "# Example 3: Create a custom state (41 binary values)\n",
        "# This is the initial state: everything at location A\n",
        "custom_state = (1,0,0,0, 1,0,0,0, 1,0,0,0, 1,0,0,0, 1,0,0,0, \n",
        "                1,0,0,0, 1,0,0,0, 1,0,0,0, 1,0,0,0, 0,0,0,0, 0)\n",
        "action, probs = predict_action(\n",
        "    custom_state, reward_weights, feature_matrix,\n",
        "    state_to_idx, action_to_idx, idx_to_action,\n",
        "    transition_model\n",
        ")\n",
        "# print(action)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "xN5BqgVn93Bk"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.12.3)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
