{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbM9yCSSBENR"
      },
      "source": [
        "# Dataset Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTXojY_mZlY0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature_map:  {'pot_A': 0, 'pot_B': 1, 'pot_C': 2, 'pot_D': 3, 'pan_A': 4, 'pan_B': 5, 'pan_C': 6, 'pan_D': 7, 'plate_A': 8, 'plate_B': 9, 'plate_C': 10, 'plate_D': 11, 'tomato_A': 12, 'tomato_B': 13, 'tomato_C': 14, 'tomato_D': 15, 'meat_A': 16, 'meat_B': 17, 'meat_C': 18, 'meat_D': 19, 'onion_A': 20, 'onion_B': 21, 'onion_C': 22, 'onion_D': 23, 'mushroom_A': 24, 'mushroom_B': 25, 'mushroom_C': 26, 'mushroom_D': 27, 'lettuce_A': 28, 'lettuce_B': 29, 'lettuce_C': 30, 'lettuce_D': 31, 'egg_A': 32, 'egg_B': 33, 'egg_C': 34, 'egg_D': 35, 'tomato_cut': 36, 'onion_cut': 37, 'mushroom_cut': 38, 'lettuce_cut': 39, 'stove_on': 40}\n",
            "Generating 1 state-aware demonstrations...\n",
            "[1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0]\n",
            "[0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0]\n",
            "[0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0]\n",
            "[0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1\n",
            " 0 0 0 0]\n",
            "[0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1\n",
            " 0 0 0 0]\n",
            "[0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1\n",
            " 0 0 0 0]\n",
            "[0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1\n",
            " 1 0 0 0]\n",
            "[0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1\n",
            " 1 0 0 0]\n",
            "[0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1\n",
            " 1 0 0 1]\n",
            "[0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1\n",
            " 1 0 0 1]\n",
            "[0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1\n",
            " 1 0 0 1]\n",
            "Done! Saved 1 trajectories to demonstrations_with_states.txt\n",
            "State Vector Size: 41\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "DATASET_SIZE = 1\n",
        "OUTPUT_FILE = \"demonstrations_with_states.txt\"\n",
        "\n",
        "# Feature Definitions: track location (A,B,C,D) and cut status\n",
        "ITEMS = [\"pot\", \"pan\", \"plate\", \"tomato\", \"meat\", \"onion\", \"mushroom\", \"lettuce\", \"egg\"]\n",
        "CUTTABLES = [\"tomato\", \"onion\", \"mushroom\", \"lettuce\"]\n",
        "LOCATIONS = [\"A\", \"B\", \"C\", \"D\"]\n",
        "\n",
        "class StateTracker:\n",
        "    def __init__(self):\n",
        "        self.feature_map = {}\n",
        "        idx = 0\n",
        "        self.num = 20\n",
        "        \n",
        "        # Location features: e.g., \"pot_A\": 0, \"pot_B\": 1...\n",
        "        for item in ITEMS:\n",
        "            for loc in LOCATIONS:\n",
        "                self.feature_map[f\"{item}_{loc}\"] = idx\n",
        "                idx += 1\n",
        "        \n",
        "        # Cut status features: e.g., \"tomato_cut\": 36...\n",
        "        for item in CUTTABLES:\n",
        "            self.feature_map[f\"{item}_cut\"] = idx\n",
        "            idx += 1\n",
        "        \n",
        "        # Global status\n",
        "        self.feature_map[\"stove_on\"] = idx\n",
        "        self.n_features = idx + 1\n",
        "        self.reset()\n",
        "        print(\"Feature_map: \", self.feature_map)\n",
        "    \n",
        "    def reset(self):\n",
        "        \"\"\"Reset state to initial conditions (everything at A, raw, stove off)\"\"\"\n",
        "        self.current_state = np.zeros(self.n_features, dtype=int)\n",
        "        for item in ITEMS:\n",
        "            self.set_feature(f\"{item}_A\", 1)\n",
        "    \n",
        "    def set_feature(self, key, value):\n",
        "        if key in self.feature_map:\n",
        "            self.current_state[self.feature_map[key]] = value\n",
        "    \n",
        "    def get_feature(self, key):\n",
        "        return self.current_state[self.feature_map[key]] if key in self.feature_map else 0\n",
        "    \n",
        "    def get_state_vector(self):\n",
        "        if self.num:\n",
        "            print(self.current_state)\n",
        "            self.num -= 1\n",
        "        return self.current_state.copy()\n",
        "    \n",
        "    def apply_action(self, action_str):\n",
        "        \"\"\"Parse action string and update internal state vector\"\"\"\n",
        "        if action_str.startswith(\"move\"):\n",
        "            # Parse \"move (item origin to dest)\"\n",
        "            content = action_str[action_str.find(\"(\")+1 : action_str.find(\")\")]\n",
        "            item, origin, _, dest = content.split()\n",
        "            self.set_feature(f\"{item}_{origin}\", 0)\n",
        "            self.set_feature(f\"{item}_{dest}\", 1)\n",
        "        elif action_str.startswith(\"cut\"):\n",
        "            # Parse \"cut (item loc)\"\n",
        "            content = action_str[action_str.find(\"(\")+1 : action_str.find(\")\")]\n",
        "            item = content.split()[0]\n",
        "            self.set_feature(f\"{item}_cut\", 1)\n",
        "        elif action_str.startswith(\"turn_on\"):\n",
        "            self.set_feature(\"stove_on\", 1)\n",
        "        elif action_str.startswith(\"turn_off\"):\n",
        "            self.set_feature(\"stove_on\", 0)\n",
        "\n",
        "class RecipeGenerator:\n",
        "    def __init__(self):\n",
        "        self.demos = []\n",
        "        self.tracker = StateTracker()\n",
        "    \n",
        "    def _record_trajectory(self, actions):\n",
        "        \"\"\"Run actions through state tracker and record (state, action) pairs\"\"\"\n",
        "        self.tracker.reset()\n",
        "        trajectory = []\n",
        "        for action in actions:\n",
        "            state_vector = self.tracker.get_state_vector().tolist()\n",
        "            trajectory.append({\"state\": state_vector, \"action\": action})\n",
        "            self.tracker.apply_action(action)\n",
        "        \n",
        "        final_state_vector = self.tracker.get_state_vector().tolist()\n",
        "        trajectory.append({\"state\": final_state_vector, \"action\": \"stop\"})\n",
        "        self.demos.append(trajectory)\n",
        "    \n",
        "    # Recipe Definitions\n",
        "    def generate_tomato_soup(self):         return [\"move (pot A to C)\", \"move (tomato A to B)\", \"cut (tomato B)\", \"move (tomato B to C)\", \"turn_on (stove C)\", \"move (plate A to D)\", \"move (pot C to D)\"]\n",
        "    def generate_grilled_steak(self):       return [\"move (pan A to C)\", \"move (meat A to C)\", \"turn_on (stove C)\", \"move (plate A to B)\", \"turn_off (stove C)\", \"move (meat C to B)\", \"move (plate B to D)\"]\n",
        "    def generate_mushroom_stew(self):       return [\"move (pot A to C)\", \"move (mushroom A to B)\", \"cut (mushroom B)\", \"move (mushroom B to C)\", \"move (onion A to B)\", \"cut (onion B)\", \"move (onion B to C)\", \"turn_on (stove C)\", \"move (plate A to D)\", \"move (pot C to D)\"]\n",
        "    def generate_salad(self):               return [\"move (lettuce A to B)\", \"cut (lettuce B)\", \"move (onion A to B)\", \"cut (onion B)\", \"move (plate A to B)\", \"move (plate B to D)\"]\n",
        "    def generate_burger(self):              return [\"move (pan A to C)\", \"move (meat A to C)\", \"turn_on (stove C)\", \"move (lettuce A to B)\", \"cut (lettuce B)\", \"turn_off (stove C)\", \"move (meat C to B)\", \"move (plate A to B)\", \"move (plate B to D)\"]\n",
        "    def generate_boiled_eggs(self):         return [\"move (pot A to C)\", \"move (egg A to C)\", \"turn_on (stove C)\", \"turn_off (stove C)\", \"move (plate A to C)\", \"move (plate C to D)\"]\n",
        "    def generate_tomato_onion_soup_1(self):         return [\"move (pot A to C)\", \"move (tomato A to B)\", \"cut (tomato B)\", \"move (tomato B to C)\", \"move (onion A to B)\", \"cut (onion B)\", \"move (onion B to C)\", \"turn_on (stove C)\", \"move (plate A to D)\", \"move (pot C to D)\"]\n",
        "    def generate_tomato_onion_soup_2(self):         return [\"move (pot A to C)\", \"move (onion A to B)\", \"cut (onion B)\", \"move (onion B to C)\", \"move (tomato A to B)\", \"cut (tomato B)\", \"move (tomato B to C)\", \"turn_on (stove C)\", \"move (plate A to D)\", \"move (pot C to D)\"]\n",
        "\n",
        "    def generate_random_dataset(self, count):\n",
        "        # available_recipes = [self.generate_tomato_soup, self.generate_grilled_steak, self.generate_mushroom_stew, self.generate_salad, self.generate_burger, self.generate_boiled_eggs]\n",
        "        available_recipes = [self.generate_tomato_onion_soup_1]\n",
        "        print(f\"Generating {count} state-aware demonstrations...\")\n",
        "        for i in range(count):\n",
        "            recipe_func = random.choice(available_recipes)\n",
        "            actions = recipe_func()\n",
        "            self._record_trajectory(actions)\n",
        "    \n",
        "    def save_to_file(self):\n",
        "        # Save as JSON Lines (each line is a full trajectory object)\n",
        "        with open(OUTPUT_FILE, \"w\") as f:\n",
        "            for demo in self.demos:\n",
        "                f.write(json.dumps(demo) + \"\\n\")\n",
        "        print(f\"Done! Saved {len(self.demos)} trajectories to {OUTPUT_FILE}\")\n",
        "        print(f\"State Vector Size: {self.tracker.n_features}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    gen = RecipeGenerator()\n",
        "    gen.generate_random_dataset(DATASET_SIZE)\n",
        "    gen.save_to_file()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeZ5ql8i7Zaj"
      },
      "source": [
        "# IRL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNjxi8mm7Mqz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1 demonstrations\n",
            "Number of unique actions: 11\n",
            "Actions: ['cut (onion B)', 'cut (tomato B)', 'move (onion A to B)', 'move (onion B to C)', 'move (plate A to D)']...\n",
            "state_to_idx: {(0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1): 0, (0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1): 1, (0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0): 2, (0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1): 3, (0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0): 4, (0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0): 5, (0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0): 6, (0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0): 7, (0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0): 8, (0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0): 9, (1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0): 10}\n",
            "idx_to_state: {0: (0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1), 1: (0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1), 2: (0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0), 3: (0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1), 4: (0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0), 5: (0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0), 6: (0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0), 7: (0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0), 8: (0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0), 9: (0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0), 10: (1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0)}\n",
            "action_to_idx: {'cut (onion B)': 0, 'cut (tomato B)': 1, 'move (onion A to B)': 2, 'move (onion B to C)': 3, 'move (plate A to D)': 4, 'move (pot A to C)': 5, 'move (pot C to D)': 6, 'move (tomato A to B)': 7, 'move (tomato B to C)': 8, 'stop': 9, 'turn_on (stove C)': 10}\n",
            "idx_to_action: {0: 'cut (onion B)', 1: 'cut (tomato B)', 2: 'move (onion A to B)', 3: 'move (onion B to C)', 4: 'move (plate A to D)', 5: 'move (pot A to C)', 6: 'move (pot C to D)', 7: 'move (tomato A to B)', 8: 'move (tomato B to C)', 9: 'stop', 10: 'turn_on (stove C)'}\n",
            "Number of unique states: 11\n",
            "Feature matrix shape: (11, 18)\n",
            "\n",
            "Running Maximum Entropy IRL...\n",
            "\n",
            "Found 11 unique (state, action) pairs\n",
            "Empirical feature expectation norm: 61.4282\n",
            "Iteration 0: Gradient norm = 0.000000, Reward range = [-0.00, 0.00], Best = 0.000000, LR = 0.0100\n",
            "Converged at iteration 0\n",
            "\n",
            "Final best gradient norm: 0.000000\n",
            "\n",
            "=== Results ===\n",
            "Learned reward weights shape: (18,)\n",
            "Reward weight statistics:\n",
            "  Mean: -0.0000\n",
            "  Std: 0.0000\n",
            "  Min: -0.0000\n",
            "  Max: 0.0000\n",
            "\n",
            "Recovered rewards statistics:\n",
            "  Mean: -0.0000\n",
            "  Std: 0.0000\n",
            "  Min: -0.0000\n",
            "  Max: -0.0000\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "\n",
        "def load_demonstrations(filepath):\n",
        "    \"\"\"Load demonstrations from JSON file\"\"\"\n",
        "    with open(filepath, 'r') as f:\n",
        "        demo_lists = [json.loads(line) for line in f.read().strip().split('\\n')]\n",
        "    \n",
        "    demonstrations = []\n",
        "    all_actions = set()\n",
        "    \n",
        "    for demo in demo_lists:\n",
        "        trajectory = [(tuple(step['state']), step['action']) for step in demo]\n",
        "        demonstrations.append(trajectory)\n",
        "        all_actions.update(step['action'] for step in demo)\n",
        "    \n",
        "    return demonstrations, sorted(list(all_actions))\n",
        "\n",
        "def create_state_action_mappings(demonstrations, unique_actions):\n",
        "    \"\"\"Create bidirectional mappings between states/actions and indices\"\"\"\n",
        "    # Collect unique states\n",
        "    unique_states = set()\n",
        "    for trajectory in demonstrations:\n",
        "        unique_states.update(state for state, _ in trajectory)\n",
        "    \n",
        "    # Create mappings\n",
        "    state_to_idx = {state: idx for idx, state in enumerate(sorted(unique_states))}\n",
        "    idx_to_state = {idx: state for state, idx in state_to_idx.items()}\n",
        "    action_to_idx = {action: idx for idx, action in enumerate(unique_actions)}\n",
        "    idx_to_action = {idx: action for action, idx in action_to_idx.items()}\n",
        "    \n",
        "    print(\"state_to_idx:\", state_to_idx)\n",
        "    print(\"idx_to_state:\", idx_to_state)\n",
        "    print(\"action_to_idx:\", action_to_idx)\n",
        "    print(\"idx_to_action:\", idx_to_action)\n",
        "    \n",
        "    return state_to_idx, idx_to_state, action_to_idx, idx_to_action\n",
        "\n",
        "def create_feature_matrix(idx_to_state):\n",
        "    \"\"\"\n",
        "    Create task-discriminative features for kitchen environment.\n",
        "    State vector layout (41 dims):\n",
        "    - Indices 0-35: Object locations (9 objects Ã— 4 locations)\n",
        "    - Indices 36-39: Cut status (tomato, onion, mushroom, lettuce)\n",
        "    - Index 40: Stove on/off\n",
        "    \"\"\"\n",
        "    features = []\n",
        "    \n",
        "    for idx in range(len(idx_to_state)):\n",
        "        state = np.array(idx_to_state[idx])\n",
        "        feat = []\n",
        "        \n",
        "        # Features 0-3: Items at each location\n",
        "        for loc in range(4):\n",
        "            feat.append(sum(state[item*4 + loc] for item in range(9)))\n",
        "        \n",
        "        feat.append(np.sum(state[36:40]))  # Feature 4: total cut items\n",
        "        feat.append(state[40])              # Feature 5: stove on\n",
        "        \n",
        "        # Discriminative features for different cooking tasks\n",
        "        pot_at_C, pan_at_C = state[2], state[6]\n",
        "        feat.append(pot_at_C)               # Feature 6: Pot at stove C\n",
        "        feat.append(pan_at_C)               # Feature 7: Pan at stove C\n",
        "        feat.append(state[18])              # Feature 8: Egg at stove C\n",
        "        \n",
        "        tomato_cut, tomato_at_C = state[36], state[14]\n",
        "        feat.append(tomato_cut * tomato_at_C * pot_at_C)  # Feature 9: Tomato cooking\n",
        "        \n",
        "        feat.append(state[22] * pan_at_C)   # Feature 10: Meat cooking\n",
        "        feat.append(state[37])              # Feature 11: Lettuce cut\n",
        "        feat.append(state[38])              # Feature 12: Onion cut\n",
        "        feat.append(state[25] + state[29])  # Feature 13: Vegetables at cutting board\n",
        "        feat.append(state[13])              # Feature 14: Tomato at cutting board\n",
        "        feat.append(state[40] * (pot_at_C + pan_at_C))  # Feature 15: Cooking active\n",
        "        feat.append(state[11])              # Feature 16: Plate at serving location\n",
        "        # Feature 17: Cooked food ready to serve\n",
        "        feat.append((1 - state[40]) * (pot_at_C + pan_at_C) * (state[18] + tomato_at_C + state[22]))\n",
        "        \n",
        "        features.append(feat)\n",
        "    \n",
        "    return np.array(features)\n",
        "\n",
        "def max_entropy_inverse_rl_kitchen(demonstrations, state_to_idx, action_to_idx, feature_matrix,\n",
        "                                   temperature=2.0, gamma=0.9, n_iterations=100, learning_rate=0.05):\n",
        "    \"\"\"Maximum Entropy Inverse RL for sequential kitchen tasks\"\"\"\n",
        "    n_states, n_actions, n_features = len(state_to_idx), len(action_to_idx), feature_matrix.shape[1]\n",
        "    reward_weights = np.zeros(n_features)\n",
        "    \n",
        "    # Compute empirical feature expectations from expert demonstrations\n",
        "    empirical_feature_expectations = np.zeros(n_features)\n",
        "    for trajectory in demonstrations:\n",
        "        discounted_visitation = np.zeros(n_states)\n",
        "        for t, (state, action) in enumerate(trajectory):\n",
        "            discounted_visitation[state_to_idx[state]] += (gamma ** t)\n",
        "        empirical_feature_expectations += discounted_visitation @ feature_matrix\n",
        "    empirical_feature_expectations /= len(demonstrations)\n",
        "    \n",
        "    # Build transition model from demonstrations\n",
        "    transition_model = {}  # (s_idx, a_idx) -> s_next_idx\n",
        "    state_action_pairs = set()\n",
        "    \n",
        "    for trajectory in demonstrations:\n",
        "        for i in range(len(trajectory)):\n",
        "            state, action = trajectory[i]\n",
        "            s_idx, a_idx = state_to_idx[state], action_to_idx[action]\n",
        "            state_action_pairs.add((s_idx, a_idx))\n",
        "            if i < len(trajectory) - 1:\n",
        "                transition_model[(s_idx, a_idx)] = state_to_idx[trajectory[i + 1][0]]\n",
        "    \n",
        "    print(f\"\\nFound {len(state_action_pairs)} unique (state, action) pairs\")\n",
        "    print(f\"Empirical feature expectation norm: {np.linalg.norm(empirical_feature_expectations):.4f}\")\n",
        "    \n",
        "    # Gradient descent with adaptive learning rate\n",
        "    best_diff = float('inf')\n",
        "    patience = 0\n",
        "    best_weights = reward_weights.copy()\n",
        "    momentum = np.zeros(n_features)\n",
        "    momentum_beta = 0.9\n",
        "    \n",
        "    for iteration in range(n_iterations):\n",
        "        current_lr = learning_rate * (0.95 ** (patience // 5))\n",
        "        rewards = feature_matrix @ reward_weights\n",
        "        \n",
        "        # Backward value iteration\n",
        "        q_values = {}  # (s_idx, a_idx) -> Q-value\n",
        "        values = rewards.copy()\n",
        "        \n",
        "        for _ in range(30):\n",
        "            new_values = rewards.copy()\n",
        "            \n",
        "            # Update Q-values\n",
        "            for (s_idx, a_idx) in state_action_pairs:\n",
        "                if (s_idx, a_idx) in transition_model:\n",
        "                    q_values[(s_idx, a_idx)] = rewards[s_idx] + gamma * values[transition_model[(s_idx, a_idx)]]\n",
        "                else:\n",
        "                    q_values[(s_idx, a_idx)] = rewards[s_idx]  # Terminal action\n",
        "            \n",
        "            # Update values using soft-max\n",
        "            for s_idx in range(n_states):\n",
        "                available_qs = [q_values[(s_idx, a_idx)] for a_idx in range(n_actions) if (s_idx, a_idx) in q_values]\n",
        "                if available_qs:\n",
        "                    available_qs = np.array(available_qs)\n",
        "                    max_q = np.max(available_qs)\n",
        "                    new_values[s_idx] = max_q + temperature * np.log(np.sum(np.exp((available_qs - max_q) / temperature)))\n",
        "            \n",
        "            if np.max(np.abs(new_values - values)) < 1e-6:\n",
        "                break\n",
        "            values = new_values\n",
        "        \n",
        "        # Compute soft policy from Q-values\n",
        "        policy = {}\n",
        "        for s_idx in range(n_states):\n",
        "            state_qs = [(a_idx, q_values[(s_idx, a_idx)]) for a_idx in range(n_actions) if (s_idx, a_idx) in q_values]\n",
        "            if state_qs:\n",
        "                state_actions, state_q_vals = zip(*state_qs)\n",
        "                state_q_vals = np.array(state_q_vals)\n",
        "                max_q = np.max(state_q_vals)\n",
        "                probs = np.exp((state_q_vals - max_q) / temperature)\n",
        "                probs /= np.sum(probs)\n",
        "                for a_idx, prob in zip(state_actions, probs):\n",
        "                    policy[(s_idx, a_idx)] = prob\n",
        "        \n",
        "        # Monte Carlo estimation of expected feature counts\n",
        "        expected_feature_counts = np.zeros(n_features)\n",
        "        for _ in range(100):  # Sample 100 trajectories\n",
        "            s_idx = state_to_idx[demonstrations[np.random.randint(len(demonstrations))][0][0]]\n",
        "            traj_features = np.zeros(n_features)\n",
        "            \n",
        "            for t in range(20):  # Max trajectory length\n",
        "                traj_features += (gamma ** t) * feature_matrix[s_idx]\n",
        "                \n",
        "                # Sample action according to policy\n",
        "                available = [(a_idx, policy[(s_idx, a_idx)]) for a_idx in range(n_actions) if (s_idx, a_idx) in policy]\n",
        "                if not available:\n",
        "                    break\n",
        "                \n",
        "                actions, probs = zip(*available)\n",
        "                probs = np.array(probs) / np.sum(probs)\n",
        "                a_idx = np.random.choice(actions, p=probs)\n",
        "                \n",
        "                if (s_idx, a_idx) not in transition_model:\n",
        "                    break\n",
        "                s_idx = transition_model[(s_idx, a_idx)]\n",
        "            \n",
        "            expected_feature_counts += traj_features\n",
        "        expected_feature_counts /= 100\n",
        "        \n",
        "        # Gradient update with momentum\n",
        "        gradient = empirical_feature_expectations - expected_feature_counts\n",
        "        gradient -= 0.01 * reward_weights  # L2 regularization\n",
        "        grad_norm = np.linalg.norm(gradient)\n",
        "        \n",
        "        momentum = momentum_beta * momentum + (1 - momentum_beta) * gradient\n",
        "        reward_weights += current_lr * momentum\n",
        "        \n",
        "        # Track best solution\n",
        "        if grad_norm < best_diff:\n",
        "            best_diff = grad_norm\n",
        "            best_weights = reward_weights.copy()\n",
        "            patience = 0\n",
        "        else:\n",
        "            patience += 1\n",
        "        \n",
        "        if iteration % 10 == 0:\n",
        "            print(f\"Iteration {iteration}: Gradient norm = {grad_norm:.6f}, \"\n",
        "                  f\"Reward range = [{np.min(reward_weights):.2f}, {np.max(reward_weights):.2f}], \"\n",
        "                  f\"Best = {best_diff:.6f}, LR = {current_lr:.4f}\")\n",
        "        \n",
        "        if grad_norm < 0.01 or patience > 20:\n",
        "            print(f\"Converged at iteration {iteration}\")\n",
        "            break\n",
        "    \n",
        "    print(f\"\\nFinal best gradient norm: {best_diff:.6f}\")\n",
        "    recovered_rewards = feature_matrix @ best_weights\n",
        "    return best_weights, recovered_rewards\n",
        "\n",
        "def predict_action(state, reward_weights, feature_matrix, state_to_idx, action_to_idx,\n",
        "                  idx_to_action, transition_model, temperature=2.0, gamma=0.9):\n",
        "    \"\"\"Predict most likely action from a given state\"\"\"\n",
        "    if state not in state_to_idx:\n",
        "        print(f\"Warning: State not seen during training\")\n",
        "        return None, {}\n",
        "    \n",
        "    s_idx = state_to_idx[state]\n",
        "    rewards = feature_matrix @ reward_weights\n",
        "    \n",
        "    # Compute Q-values for all valid actions\n",
        "    valid_actions = []\n",
        "    q_values_list = []\n",
        "    \n",
        "    for a_idx in range(len(action_to_idx)):\n",
        "        if (s_idx, a_idx) in transition_model:\n",
        "            s_next_idx = transition_model[(s_idx, a_idx)]\n",
        "            q_value = rewards[s_idx] + gamma * rewards[s_next_idx]\n",
        "            valid_actions.append(a_idx)\n",
        "            q_values_list.append(q_value)\n",
        "    \n",
        "    if not valid_actions:\n",
        "        print(f\"Warning: No valid actions from this state!\")\n",
        "        return None, {}\n",
        "    \n",
        "    # Compute softmax policy\n",
        "    q_values_array = np.array(q_values_list)\n",
        "    max_q = np.max(q_values_array)\n",
        "    action_probs_array = np.exp((q_values_array - max_q) / temperature)\n",
        "    action_probs_array /= np.sum(action_probs_array)\n",
        "    \n",
        "    # Create action probability dictionary\n",
        "    action_probs = {idx_to_action[a_idx]: prob for a_idx, prob in zip(valid_actions, action_probs_array)}\n",
        "    \n",
        "    # Get most likely action\n",
        "    best_action_idx = valid_actions[np.argmax(action_probs_array)]\n",
        "    predicted_action = idx_to_action[best_action_idx]\n",
        "    \n",
        "    return predicted_action, action_probs\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Load demonstrations\n",
        "    demonstrations, unique_actions = load_demonstrations('demonstrations_with_states.txt')\n",
        "    print(f\"Loaded {len(demonstrations)} demonstrations\")\n",
        "    print(f\"Number of unique actions: {len(unique_actions)}\")\n",
        "    print(f\"Actions: {unique_actions[:5]}...\")\n",
        "    \n",
        "    # Create mappings\n",
        "    state_to_idx, idx_to_state, action_to_idx, idx_to_action = \\\n",
        "        create_state_action_mappings(demonstrations, unique_actions)\n",
        "    print(f\"Number of unique states: {len(state_to_idx)}\")\n",
        "    \n",
        "    # Build transition model\n",
        "    transition_model = {}\n",
        "    for trajectory in demonstrations:\n",
        "        for i in range(len(trajectory)):\n",
        "            state, action = trajectory[i]\n",
        "            s_idx, a_idx = state_to_idx[state], action_to_idx[action]\n",
        "            if i < len(trajectory) - 1:\n",
        "                transition_model[(s_idx, a_idx)] = state_to_idx[trajectory[i + 1][0]]\n",
        "    \n",
        "    # Create feature matrix\n",
        "    feature_matrix = create_feature_matrix(idx_to_state)\n",
        "    print(f\"Feature matrix shape: {feature_matrix.shape}\")\n",
        "    \n",
        "    # Run IRL\n",
        "    print(\"\\nRunning Maximum Entropy IRL...\")\n",
        "    reward_weights, recovered_rewards = max_entropy_inverse_rl_kitchen(\n",
        "        demonstrations=demonstrations,\n",
        "        state_to_idx=state_to_idx,\n",
        "        action_to_idx=action_to_idx,\n",
        "        feature_matrix=feature_matrix,\n",
        "        temperature=1.0,\n",
        "        gamma=0.95,\n",
        "        n_iterations=5,\n",
        "        learning_rate=0.01\n",
        "    )\n",
        "    \n",
        "    print(\"\\n=== Results ===\")\n",
        "    print(f\"Learned reward weights shape: {reward_weights.shape}\")\n",
        "    print(f\"Reward weight statistics:\")\n",
        "    print(f\"  Mean: {np.mean(reward_weights):.4f}\")\n",
        "    print(f\"  Std: {np.std(reward_weights):.4f}\")\n",
        "    print(f\"  Min: {np.min(reward_weights):.4f}\")\n",
        "    print(f\"  Max: {np.max(reward_weights):.4f}\")\n",
        "    \n",
        "    print(f\"\\nRecovered rewards statistics:\")\n",
        "    print(f\"  Mean: {np.mean(recovered_rewards):.4f}\")\n",
        "    print(f\"  Std: {np.std(recovered_rewards):.4f}\")\n",
        "    print(f\"  Min: {np.min(recovered_rewards):.4f}\")\n",
        "    print(f\"  Max: {np.max(recovered_rewards):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'cut (onion B)': 0, 'cut (tomato B)': 1, 'move (onion A to B)': 2, 'move (onion B to C)': 3, 'move (plate A to D)': 4, 'move (pot A to C)': 5, 'move (pot C to D)': 6, 'move (tomato A to B)': 7, 'move (tomato B to C)': 8, 'stop': 9, 'turn_on (stove C)': 10}\n"
          ]
        }
      ],
      "source": [
        "print(action_to_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Actual: move (pot A to C) \n",
            " Predicted: move (pot A to C) \n",
            "\n",
            "Actual: move (tomato A to B) \n",
            " Predicted: move (tomato A to B) \n",
            "\n",
            "Actual: cut (tomato B) \n",
            " Predicted: cut (tomato B) \n",
            "\n",
            "Actual: move (tomato B to C) \n",
            " Predicted: move (tomato B to C) \n",
            "\n",
            "Actual: move (onion A to B) \n",
            " Predicted: move (onion A to B) \n",
            "\n",
            "Actual: cut (onion B) \n",
            " Predicted: cut (onion B) \n",
            "\n",
            "Actual: move (onion B to C) \n",
            " Predicted: move (onion B to C) \n",
            "\n",
            "Actual: turn_on (stove C) \n",
            " Predicted: turn_on (stove C) \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test predictions\n",
        "# Example 1: Test the 5th step of trajectory 2\n",
        "# state = demonstrations[2][4][0]\n",
        "# action, probs = predict_action(state, reward_weights, feature_matrix,\n",
        "#                                state_to_idx, action_to_idx, idx_to_action, transition_model)\n",
        "# print(f\"Predicted: {action}\")\n",
        "# print(f\"Probabilities: {probs}\", \"\\n\\n\")\n",
        "\n",
        "# Example 2: Test first 6 steps of first 3 trajectories\n",
        "for i in range(1):\n",
        "    for j in range(8):\n",
        "        state = demonstrations[i][j][0]\n",
        "        action, probs = predict_action(state, reward_weights, feature_matrix,\n",
        "                                      state_to_idx, action_to_idx, idx_to_action, transition_model)\n",
        "        print(f\"Actual: {demonstrations[i][j][1]} \\n Predicted: {action} \\n\")\n",
        "    print(\"\\n\\n\\n\")\n",
        "\n",
        "# Example 3: Create a custom state (initial state: everything at location A)\n",
        "custom_state = (1,0,0,0, 1,0,0,0, 1,0,0,0, 1,0,0,0, 1,0,0,0,\n",
        "                1,0,0,0, 1,0,0,0, 1,0,0,0, 1,0,0,0, 0,0,0,0, 0)\n",
        "action, probs = predict_action(custom_state, reward_weights, feature_matrix,\n",
        "                               state_to_idx, action_to_idx, idx_to_action, transition_model)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "xN5BqgVn93Bk"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.12.3)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
