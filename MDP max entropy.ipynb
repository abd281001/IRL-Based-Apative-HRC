{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pS-5IGNLmglA"
   },
   "source": [
    "# RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1762296034126,
     "user": {
      "displayName": "El Chupacabra",
      "userId": "00046636496346319117"
     },
     "user_tz": 300
    },
    "id": "FUQIBPOS2JL5",
    "outputId": "86d6be5d-d355-43a9-c1ae-8bc07178d344"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal Policy (probability of each action in each state):\n",
      "[[0.75980863 0.24019137]\n",
      " [0.09539876 0.90460124]\n",
      " [0.40078342 0.59921658]]\n",
      "\n",
      "Q-Values:\n",
      "[[33.0680211  31.9163905 ]\n",
      " [31.4163905  33.66581918]\n",
      " [32.16581918 32.5680211 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def max_entropy_policy_optimization(states, actions, rewards, transitions, temperature=1.0, gamma=0.99, n_iterations=1000, tolerance=1e-6):\n",
    "    \"\"\"\n",
    "    Find optimal policy using Maximum Entropy Reinforcement Learning (Soft Q-Learning).\n",
    "\n",
    "    Args:\n",
    "        states: list/array of state indices [0, 1, 2, ..., n_states-1]\n",
    "        actions: list/array of action indices [0, 1, 2, ..., n_actions-1]\n",
    "        rewards: dict or 2D array, rewards[state][action] = reward value\n",
    "        transitions: dict or 3D array, transitions[state][action] = next_state distribution\n",
    "        temperature: float, controls exploration (higher = more exploration)\n",
    "        gamma: float, discount factor for future rewards\n",
    "        n_iterations: int, maximum number of value iteration steps\n",
    "        tolerance: float, convergence threshold\n",
    "\n",
    "    Returns:\n",
    "        policy: 2D array of shape (n_states, n_actions) with action probabilities\n",
    "        q_values: 2D array of shape (n_states, n_actions) with soft Q-values\n",
    "    \"\"\"\n",
    "    # Step 1: Initialize dimensions\n",
    "    n_states = len(states)  # Total number of states in the MDP\n",
    "    n_actions = len(actions)  # Total number of actions available\n",
    "\n",
    "    # Step 2: Initialize Q-values (soft Q-function) to zeros\n",
    "    q_values = np.zeros((n_states, n_actions))                                                                  # Q[s, a] represents expected return from taking action a in state s\n",
    "    # Step 3: Value iteration loop to learn optimal Q-values\n",
    "    for iteration in range(n_iterations):\n",
    "\n",
    "        q_old = q_values.copy()                                                                                 # Store previous Q-values to check convergence\n",
    "        # Step 4: Update Q-value for each state-action pair\n",
    "        for s in states:\n",
    "            for a in actions:\n",
    "                # Step 4a: Get immediate reward for this state-action pair\n",
    "                r = rewards[s][a] if isinstance(rewards, dict) else rewards[s, a]\n",
    "                # Step 4b: Get next state distribution (could be deterministic or stochastic)\n",
    "                next_state_dist = transitions[s][a] if isinstance(transitions, dict) else transitions[s, a]\n",
    "                # Step 4c: Calculate expected soft value of next state\n",
    "                expected_next_value = 0                                                                         # This is where maximum entropy comes in\n",
    "\n",
    "                # Handle both deterministic (single next state) and stochastic transitions\n",
    "                if isinstance(next_state_dist, (int, np.integer)):\n",
    "                    s_next = next_state_dist                                                                    # Deterministic transition to single next state\n",
    "                    # Step 4d: Compute soft value (max-ent value function)\n",
    "                    soft_value = temperature * np.log(np.sum(np.exp(q_old[s_next] / temperature)))              # V(s) = temperature * log(sum(exp(Q(s,a)/temperature)))\n",
    "                    expected_next_value = soft_value\n",
    "                else:\n",
    "                    # Stochastic transition over multiple next states\n",
    "                    for s_next, prob in enumerate(next_state_dist):\n",
    "                        if prob > 0:\n",
    "                            soft_value = temperature * np.log(np.sum(np.exp(q_old[s_next] / temperature)))      # Compute soft value for this next state\n",
    "                            expected_next_value += prob * soft_value                                            # Weight by transition probability\n",
    "\n",
    "                # Step 4e: Bellman backup with soft value function\n",
    "                # Q(s,a) = r(s,a) + gamma * E[V(s')]\n",
    "                q_values[s, a] = r + gamma * expected_next_value\n",
    "\n",
    "        # Step 5: Check for convergence\n",
    "        # If Q-values haven't changed much, we've found the optimal Q-function\n",
    "        max_change = np.max(np.abs(q_values - q_old))\n",
    "        if max_change < tolerance:\n",
    "            print(f\"Converged after {iteration + 1} iterations\")\n",
    "            break\n",
    "\n",
    "    # Step 6: Extract optimal policy from Q-values using Boltzmann distribution\n",
    "    # This gives us a stochastic policy that trades off reward and entropy\n",
    "    # pi(a|s) = exp(Q(s,a)/temperature) / sum(exp(Q(s,a')/temperature))\n",
    "    policy = np.zeros((n_states, n_actions))\n",
    "    for s in states:\n",
    "        # Step 6a: Compute unnormalized probabilities using softmax\n",
    "        exp_q = np.exp(q_values[s] / temperature)\n",
    "        # Step 6b: Normalize to get valid probability distribution\n",
    "        policy[s] = exp_q / np.sum(exp_q)\n",
    "\n",
    "    # Step 7: Return the optimal maximum entropy policy and Q-values\n",
    "    return policy, q_values\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Define a simple 3-state, 2-action MDP\n",
    "    states = [0, 1, 2]\n",
    "    actions = [0, 1]\n",
    "\n",
    "    # Reward function: rewards[state][action]\n",
    "    rewards = {\n",
    "        0: [1.0, 0.5],   # State 0: action 0 gives reward 1.0, action 1 gives 0.5\n",
    "        1: [0.0, 2.0],   # State 1: action 0 gives reward 0.0, action 1 gives 2.0\n",
    "        2: [0.5, 0.5]    # State 2: both actions give reward 0.5\n",
    "    }\n",
    "\n",
    "    # Transition function: transitions[state][action] = next_state\n",
    "    # (deterministic transitions for simplicity)\n",
    "    transitions = {\n",
    "        0: [1, 2],  # State 0: action 0 → state 1, action 1 → state 2\n",
    "        1: [2, 0],  # State 1: action 0 → state 2, action 1 → state 0\n",
    "        2: [0, 1]   # State 2: action 0 → state 0, action 1 → state 1\n",
    "    }\n",
    "\n",
    "    # Find optimal policy\n",
    "    policy, q_values = max_entropy_policy_optimization(\n",
    "        states, actions, rewards, transitions,\n",
    "        temperature=1.0,  # Controls exploration level\n",
    "        gamma=0.95,       # Discount factor\n",
    "        n_iterations=100\n",
    "    )\n",
    "\n",
    "    print(\"\\nOptimal Policy (probability of each action in each state):\")\n",
    "    print(policy)\n",
    "    print(\"\\nQ-Values:\")\n",
    "    print(q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yP2o4RRsmaY4"
   },
   "source": [
    "# IRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 277,
     "status": "ok",
     "timestamp": 1762467809128,
     "user": {
      "displayName": "El Chupacabra",
      "userId": "00046636496346319117"
     },
     "user_tz": 300
    },
    "id": "YN3pixcZmYj8",
    "outputId": "e2cec008-6c4f-4ea3-ebe1-e643c85090ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10: Feature expectation difference = 1.421723\n",
      "Iteration 20: Feature expectation difference = 1.352766\n",
      "Iteration 30: Feature expectation difference = 1.309204\n",
      "Iteration 40: Feature expectation difference = 1.281922\n",
      "Iteration 50: Feature expectation difference = 1.265306\n",
      "\n",
      "Learned Reward Weights:\n",
      "[3.59460848 5.69581094]\n",
      "\n",
      "Recovered Rewards for each state:\n",
      "State 0: 2.848\n",
      "State 1: 5.065\n",
      "State 2: 4.225\n",
      "State 3: 9.290\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def max_entropy_inverse_rl(states, actions, demonstrations, transitions,\n",
    "                           feature_matrix, temperature=1.0, gamma=0.99,\n",
    "                           n_iterations=100, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Find reward function from expert demonstrations using Maximum Entropy Inverse RL.\n",
    "\n",
    "    Args:\n",
    "        states: list/array of state indices [0, 1, 2, ..., n_states-1]\n",
    "        actions: list/array of action indices [0, 1, 2, ..., n_actions-1]\n",
    "        demonstrations: list of trajectories, each trajectory is list of (state, action) tuples\n",
    "        transitions: dict or 3D array, transitions[state][action] = next_state\n",
    "        feature_matrix: 2D array of shape (n_states, n_features), features for each state\n",
    "        temperature: float, controls exploration assumption in expert policy\n",
    "        gamma: float, discount factor for future rewards\n",
    "        n_iterations: int, number of gradient descent iterations\n",
    "        learning_rate: float, step size for gradient updates\n",
    "\n",
    "    Returns:\n",
    "        reward_weights: 1D array of learned reward function weights\n",
    "        recovered_rewards: 1D array of recovered reward for each state\n",
    "    \"\"\"\n",
    "    # Step 1: Initialize dimensions\n",
    "    n_states = len(states)  # Total number of states in the MDP\n",
    "    n_actions = len(actions)  # Total number of actions available\n",
    "    n_features = feature_matrix.shape[1]  # Number of features describing each state\n",
    "\n",
    "    # Step 2: Initialize reward weights randomly\n",
    "    # The reward function is parameterized as: r(s) = w^T * φ(s)\n",
    "    # where w are weights and φ(s) are features of state s\n",
    "    reward_weights = np.random.randn(n_features) * 0.01\n",
    "\n",
    "    # Step 3: Compute empirical feature expectations from expert demonstrations\n",
    "    # This represents what features the expert actually visits\n",
    "    # μ_E = (1/N) * Σ_trajectories Σ_t γ^t * φ(s_t)\n",
    "    empirical_feature_expectations = np.zeros(n_features)\n",
    "    total_steps = 0  # Count total timesteps across all demonstrations\n",
    "\n",
    "    # Step 3a: Iterate through each expert trajectory\n",
    "    for trajectory in demonstrations:\n",
    "        # Step 3b: Accumulate discounted features along the trajectory\n",
    "        for t, (state, action) in enumerate(trajectory):\n",
    "            # Discount factor applied based on timestep\n",
    "            discount = gamma ** t\n",
    "            # Add discounted features of this state to expectation\n",
    "            empirical_feature_expectations += discount * feature_matrix[state]\n",
    "            total_steps += 1\n",
    "\n",
    "    # Step 3c: Normalize by number of timesteps to get average\n",
    "    empirical_feature_expectations /= len(demonstrations)\n",
    "\n",
    "    # Step 4: Gradient descent loop to learn reward weights\n",
    "    for iteration in range(n_iterations):\n",
    "        # Step 4a: Compute rewards from current weight estimate\n",
    "        # r(s) = w^T * φ(s) for each state\n",
    "        rewards = feature_matrix @ reward_weights  # Matrix multiplication: (n_states, n_features) @ (n_features,) = (n_states,)\n",
    "\n",
    "        # Step 4b: Expand rewards to state-action pairs\n",
    "        # Since reward is typically state-based, same reward for all actions in a state\n",
    "        rewards_sa = np.tile(rewards, (n_actions, 1)).T  # Shape: (n_states, n_actions)\n",
    "\n",
    "        # Step 4c: Run forward RL to get policy under current reward estimate\n",
    "        # This uses soft Q-learning (maximum entropy RL) to find what policy\n",
    "        # would be optimal if the current reward estimate were correct\n",
    "        q_values = np.zeros((n_states, n_actions))\n",
    "\n",
    "        # Step 4d: Soft value iteration to compute Q-values\n",
    "        for _ in range(50):  # Inner loop for value iteration convergence\n",
    "            q_old = q_values.copy()\n",
    "\n",
    "            # Update Q-value for each state-action pair\n",
    "            for s in states:\n",
    "                for a in actions:\n",
    "                    # Get immediate reward for this state-action pair\n",
    "                    r = rewards_sa[s, a]\n",
    "\n",
    "                    # Get next state from transition function\n",
    "                    next_state = transitions[s][a] if isinstance(transitions, dict) else transitions[s, a]\n",
    "\n",
    "                    # Handle deterministic transitions\n",
    "                    if isinstance(next_state, (int, np.integer)):\n",
    "                        s_next = next_state\n",
    "                        # Compute soft value: V(s) = temperature * log(Σ exp(Q(s,a)/temperature))\n",
    "                        soft_value = temperature * np.log(np.sum(np.exp(q_old[s_next] / temperature)))\n",
    "                        expected_next_value = soft_value\n",
    "                    else:\n",
    "                        # Handle stochastic transitions\n",
    "                        expected_next_value = 0\n",
    "                        for s_next, prob in enumerate(next_state):\n",
    "                            if prob > 0:\n",
    "                                soft_value = temperature * np.log(np.sum(np.exp(q_old[s_next] / temperature)))\n",
    "                                expected_next_value += prob * soft_value\n",
    "\n",
    "                    # Bellman backup: Q(s,a) = r(s,a) + γ * E[V(s')]\n",
    "                    q_values[s, a] = r + gamma * expected_next_value\n",
    "\n",
    "        # Step 4e: Extract policy from Q-values using Boltzmann distribution\n",
    "        # π(a|s) = exp(Q(s,a)/temperature) / Σ_a' exp(Q(s,a')/temperature)\n",
    "        policy = np.zeros((n_states, n_actions))\n",
    "        for s in states:\n",
    "            exp_q = np.exp(q_values[s] / temperature)\n",
    "            policy[s] = exp_q / np.sum(exp_q)\n",
    "\n",
    "        # Step 4f: Compute expected feature counts under current policy\n",
    "        # This represents what features our current policy would visit\n",
    "        # We need to compute state visitation frequencies first\n",
    "\n",
    "        # Step 4g: Compute state visitation frequencies\n",
    "        # This tells us how often each state is visited under current policy\n",
    "        # Using a simplified approach: iterative computation of discounted state frequencies\n",
    "        state_freq = np.zeros(n_states)\n",
    "\n",
    "        # Initialize with starting states from demonstrations\n",
    "        for trajectory in demonstrations:\n",
    "            if len(trajectory) > 0:\n",
    "                start_state = trajectory[0][0]\n",
    "                state_freq[start_state] += 1.0\n",
    "        state_freq /= len(demonstrations)\n",
    "\n",
    "        # Propagate frequencies forward through state transitions\n",
    "        for _ in range(20):  # Iterate to convergence\n",
    "            new_freq = state_freq.copy()\n",
    "            for s in states:\n",
    "                for a in actions:\n",
    "                    # Get transition to next state\n",
    "                    next_state = transitions[s][a] if isinstance(transitions, dict) else transitions[s, a]\n",
    "\n",
    "                    # Handle deterministic transitions\n",
    "                    if isinstance(next_state, (int, np.integer)):\n",
    "                        s_next = next_state\n",
    "                        # Accumulate frequency: how often we reach s_next from s via action a\n",
    "                        new_freq[s_next] += gamma * state_freq[s] * policy[s, a]\n",
    "                    else:\n",
    "                        # Handle stochastic transitions\n",
    "                        for s_next, prob in enumerate(next_state):\n",
    "                            if prob > 0:\n",
    "                                new_freq[s_next] += gamma * state_freq[s] * policy[s, a] * prob\n",
    "\n",
    "            state_freq = new_freq / np.sum(new_freq)  # Normalize\n",
    "\n",
    "        # Step 4h: Compute expected feature counts from state visitation frequencies\n",
    "        # μ(π) = Σ_s P(s|π) * φ(s)\n",
    "        # This is what features we expect to see under our current policy\n",
    "        expected_feature_counts = state_freq @ feature_matrix  # Shape: (n_features,)\n",
    "\n",
    "        # Step 4i: Compute gradient of the objective\n",
    "        # The gradient of maximum entropy IRL objective is:\n",
    "        # ∇_w L = μ_E - μ(π_w)\n",
    "        # We want to match expert feature expectations with policy feature expectations\n",
    "        gradient = empirical_feature_expectations - expected_feature_counts\n",
    "\n",
    "        # Step 4j: Update reward weights using gradient ascent\n",
    "        # Move weights in direction that makes policy match expert demonstrations better\n",
    "        reward_weights += learning_rate * gradient\n",
    "\n",
    "        # Step 4k: Print progress every 10 iterations\n",
    "        if (iteration + 1) % 10 == 0:\n",
    "            # Measure how close we are to matching expert behavior\n",
    "            feature_diff = np.linalg.norm(empirical_feature_expectations - expected_feature_counts)\n",
    "            print(f\"Iteration {iteration + 1}: Feature expectation difference = {feature_diff:.6f}\")\n",
    "\n",
    "    # Step 5: Compute final recovered rewards for each state\n",
    "    # r(s) = w^T * φ(s)\n",
    "    recovered_rewards = feature_matrix @ reward_weights\n",
    "\n",
    "    # Step 6: Return learned reward weights and recovered reward function\n",
    "    return reward_weights, recovered_rewards\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Define a simple 4-state, 2-action MDP\n",
    "    states = [0, 1, 2, 3]\n",
    "    actions = [0, 1]\n",
    "\n",
    "    # Transition function: transitions[state][action] = next_state\n",
    "    transitions = {\n",
    "        0: [1, 2],  # State 0: action 0 → state 1, action 1 → state 2\n",
    "        1: [2, 3],  # State 1: action 0 → state 2, action 1 → state 3\n",
    "        2: [3, 0],  # State 2: action 0 → state 3, action 1 → state 0\n",
    "        3: [0, 1]   # State 3: action 0 → state 0, action 1 → state 1\n",
    "    }\n",
    "\n",
    "    # Feature matrix: each state described by 2 features\n",
    "    # For example: [distance_to_goal, safety_level]\n",
    "    feature_matrix = np.array([\n",
    "        [0.0, 0.5],  # State 0 features\n",
    "        [0.3, 0.7],  # State 1 features\n",
    "        [0.7, 0.3],  # State 2 features\n",
    "        [1.0, 1.0]   # State 3 features (goal state with high values)\n",
    "    ])\n",
    "\n",
    "    # Expert demonstrations: list of trajectories\n",
    "    # Each trajectory is a sequence of (state, action) pairs\n",
    "    # Expert seems to prefer reaching state 3\n",
    "    demonstrations = [\n",
    "        [(0, 0), (1, 1), (3, 1)],  # Trajectory 1: 0→1→3→1\n",
    "        [(0, 1), (2, 0), (3, 0)],  # Trajectory 2: 0→2→3→0\n",
    "        [(1, 1), (3, 1), (1, 1)],  # Trajectory 3: 1→3→1→3\n",
    "    ]\n",
    "\n",
    "    # Run inverse RL to recover reward function\n",
    "    reward_weights, recovered_rewards = max_entropy_inverse_rl(\n",
    "        states, actions, demonstrations, transitions, feature_matrix,\n",
    "        temperature=1.0,\n",
    "        gamma=0.9,\n",
    "        n_iterations=50,\n",
    "        learning_rate=0.1\n",
    "    )\n",
    "\n",
    "    print(\"\\nLearned Reward Weights:\")\n",
    "    print(reward_weights)\n",
    "    print(\"\\nRecovered Rewards for each state:\")\n",
    "    for s in states:\n",
    "        print(f\"State {s}: {recovered_rewards[s]:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPp6L0ePgtW3RFrrPOFGs7c",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
