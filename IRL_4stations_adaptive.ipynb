{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbM9yCSSBENR"
      },
      "source": [
        "# Dataset Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTXojY_mZlY0"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "DATASET_SIZE = 1\n",
        "OUTPUT_FILE = \"demonstrations_with_states.txt\"\n",
        "\n",
        "# Feature Definitions: track location (A,B,C,D) and cut status\n",
        "ITEMS = [\"pot\", \"pan\", \"plate\", \"tomato\", \"meat\", \"onion\", \"mushroom\", \"lettuce\", \"egg\"]\n",
        "CUTTABLES = [\"tomato\", \"onion\", \"mushroom\", \"lettuce\"]\n",
        "LOCATIONS = [\"A\", \"B\", \"C\", \"D\"]\n",
        "\n",
        "class StateTracker:\n",
        "    def __init__(self):\n",
        "        self.feature_map = {}\n",
        "        idx = 0\n",
        "        self.num = 20\n",
        "        \n",
        "        # Location features: e.g., \"pot_A\": 0, \"pot_B\": 1...\n",
        "        for item in ITEMS:\n",
        "            for loc in LOCATIONS:\n",
        "                self.feature_map[f\"{item}_{loc}\"] = idx\n",
        "                idx += 1\n",
        "        \n",
        "        # Cut status features: e.g., \"tomato_cut\": 36...\n",
        "        for item in CUTTABLES:\n",
        "            self.feature_map[f\"{item}_cut\"] = idx\n",
        "            idx += 1\n",
        "        \n",
        "        # Global status\n",
        "        self.feature_map[\"stove_on\"] = idx\n",
        "        self.n_features = idx + 1\n",
        "        self.reset()\n",
        "        print(\"Feature_map: \", self.feature_map)\n",
        "    \n",
        "    def reset(self):\n",
        "        \"\"\"Reset state to initial conditions (everything at A, raw, stove off)\"\"\"\n",
        "        self.current_state = np.zeros(self.n_features, dtype=int)\n",
        "        for item in ITEMS:\n",
        "            self.set_feature(f\"{item}_A\", 1)\n",
        "    \n",
        "    def set_feature(self, key, value):\n",
        "        if key in self.feature_map:\n",
        "            self.current_state[self.feature_map[key]] = value\n",
        "    \n",
        "    def get_feature(self, key):\n",
        "        return self.current_state[self.feature_map[key]] if key in self.feature_map else 0\n",
        "    \n",
        "    def get_state_vector(self):\n",
        "        if self.num:\n",
        "            # print(self.current_state)\n",
        "            self.num -= 1\n",
        "        return self.current_state.copy()\n",
        "    \n",
        "    def apply_action(self, action_str):\n",
        "        \"\"\"Parse action string and update internal state vector\"\"\"\n",
        "        if action_str.startswith(\"move\"):\n",
        "            # Parse \"move (item origin to dest)\"\n",
        "            content = action_str[action_str.find(\"(\")+1 : action_str.find(\")\")]\n",
        "            item, origin, _, dest = content.split()\n",
        "            self.set_feature(f\"{item}_{origin}\", 0)\n",
        "            self.set_feature(f\"{item}_{dest}\", 1)\n",
        "        elif action_str.startswith(\"cut\"):\n",
        "            # Parse \"cut (item loc)\"\n",
        "            content = action_str[action_str.find(\"(\")+1 : action_str.find(\")\")]\n",
        "            item = content.split()[0]\n",
        "            self.set_feature(f\"{item}_cut\", 1)\n",
        "        elif action_str.startswith(\"turn_on\"):\n",
        "            self.set_feature(\"stove_on\", 1)\n",
        "        elif action_str.startswith(\"turn_off\"):\n",
        "            self.set_feature(\"stove_on\", 0)\n",
        "\n",
        "class RecipeGenerator:\n",
        "    def __init__(self):\n",
        "        self.demos = []\n",
        "        self.tracker = StateTracker()\n",
        "    \n",
        "    def _record_trajectory(self, actions):\n",
        "        \"\"\"Run actions through state tracker and record (state, action) pairs\"\"\"\n",
        "        self.tracker.reset()\n",
        "        trajectory = []\n",
        "        for action in actions:\n",
        "            state_vector = self.tracker.get_state_vector().tolist()\n",
        "            trajectory.append({\"state\": state_vector, \"action\": action})\n",
        "            self.tracker.apply_action(action)\n",
        "        \n",
        "        final_state_vector = self.tracker.get_state_vector().tolist()\n",
        "        trajectory.append({\"state\": final_state_vector, \"action\": \"stop\"})\n",
        "        self.demos.append(trajectory)\n",
        "    \n",
        "    # Recipe Definitions\n",
        "    \n",
        "    def generate_grilled_steak(self):           return [\"move (pan A to C)\", \"move (meat A to C)\", \"turn_on (stove C)\", \"move (plate A to B)\", \"turn_off (stove C)\", \"move (meat C to B)\", \"move (plate B to D)\"]\n",
        "    def generate_salad(self):                   return [\"move (lettuce A to B)\", \"cut (lettuce B)\", \"move (onion A to B)\", \"cut (onion B)\", \"move (plate A to B)\", \"move (plate B to D)\"]\n",
        "    def generate_burger(self):                  return [\"move (pan A to C)\", \"move (meat A to C)\", \"turn_on (stove C)\", \"move (lettuce A to B)\", \"cut (lettuce B)\", \"turn_off (stove C)\", \"move (meat C to B)\", \"move (plate A to B)\", \"move (plate B to D)\"]\n",
        "    def generate_boiled_eggs(self):             return [\"move (pot A to C)\", \"move (egg A to C)\", \"turn_on (stove C)\", \"turn_off (stove C)\", \"move (plate A to C)\", \"move (plate C to D)\"]\n",
        "\n",
        "    def generate_tomato_soup(self):             return [\"move (pot A to C)\", \"move (tomato A to B)\", \"cut (tomato B)\", \"move (tomato B to C)\", \"turn_on (stove C)\", \"move (plate A to D)\", \"move (pot C to D)\"]\n",
        "    def generate_mushroom_soup(self):           return [\"move (pot A to C)\", \"move (mushroom A to B)\", \"cut (mushroom B)\", \"move (mushroom B to C)\", \"move (onion A to B)\", \"cut (onion B)\", \"move (onion B to C)\", \"turn_on (stove C)\", \"move (plate A to D)\", \"move (pot C to D)\"]\n",
        "    def generate_tomato_onion_soup_1(self):     return [\"move (pot A to C)\", \"move (tomato A to B)\", \"cut (tomato B)\", \"move (tomato B to C)\", \"move (onion A to B)\", \"cut (onion B)\", \"move (onion B to C)\", \"turn_on (stove C)\", \"move (plate A to D)\", \"move (pot C to D)\"]\n",
        "    def generate_tomato_onion_soup_2(self):     return [\"move (pot A to C)\", \"move (onion A to B)\", \"cut (onion B)\", \"move (onion B to C)\", \"move (tomato A to B)\", \"cut (tomato B)\", \"move (tomato B to C)\", \"turn_on (stove C)\", \"move (plate A to D)\", \"move (pot C to D)\"]\n",
        "\n",
        "    def generate_random_dataset(self, count):\n",
        "        # available_recipes = [self.generate_tomato_soup, self.generate_grilled_steak, self.generate_mushroom_stew, self.generate_salad, self.generate_burger, self.generate_boiled_eggs]\n",
        "        available_recipes = [self.generate_tomato_onion_soup_1]\n",
        "        print(f\"Generating {count} state-aware demonstrations...\")\n",
        "        for i in range(count):\n",
        "            recipe_func = random.choice(available_recipes)\n",
        "            actions = recipe_func()\n",
        "            self._record_trajectory(actions)\n",
        "    \n",
        "    # def save_to_file(self):\n",
        "    #     # Save as JSON Lines (each line is a full trajectory object)\n",
        "    #     with open(OUTPUT_FILE, \"w\") as f:\n",
        "    #         for demo in self.demos:\n",
        "    #             f.write(json.dumps(demo) + \"\\n\")\n",
        "    #     print(f\"Done! Saved {len(self.demos)} trajectories to {OUTPUT_FILE}\")\n",
        "    #     print(f\"State Vector Size: {self.tracker.n_features}\")\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     gen = RecipeGenerator()\n",
        "#     gen.generate_random_dataset(DATASET_SIZE)\n",
        "#     gen.save_to_file()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeZ5ql8i7Zaj"
      },
      "source": [
        "# IRL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNjxi8mm7Mqz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "\n",
        "def load_demonstrations(filepath):\n",
        "    \"\"\"Load demonstrations from JSON file\"\"\"\n",
        "    with open(filepath, 'r') as f:\n",
        "        demo_lists = [json.loads(line) for line in f.read().strip().split('\\n')]\n",
        "    \n",
        "    demonstrations = []\n",
        "    all_actions = set()\n",
        "    \n",
        "    for demo in demo_lists:\n",
        "        trajectory = [(tuple(step['state']), step['action']) for step in demo]\n",
        "        demonstrations.append(trajectory)\n",
        "        all_actions.update(step['action'] for step in demo)\n",
        "    \n",
        "    return demonstrations, sorted(list(all_actions))\n",
        "\n",
        "def create_state_action_mappings(demonstrations, unique_actions):\n",
        "    \"\"\"Create bidirectional mappings between states/actions and indices\"\"\"\n",
        "    # Collect unique states\n",
        "    unique_states = set()\n",
        "    for trajectory in demonstrations:\n",
        "        unique_states.update(state for state, _ in trajectory)\n",
        "    \n",
        "    # Create mappings\n",
        "    state_to_idx = {state: idx for idx, state in enumerate(sorted(unique_states))}\n",
        "    idx_to_state = {idx: state for state, idx in state_to_idx.items()}\n",
        "    action_to_idx = {action: idx for idx, action in enumerate(unique_actions)}\n",
        "    idx_to_action = {idx: action for action, idx in action_to_idx.items()}\n",
        "    \n",
        "    print(\"state_to_idx:\", state_to_idx)\n",
        "    print(\"idx_to_state:\", idx_to_state)\n",
        "    print(\"action_to_idx:\", action_to_idx)\n",
        "    print(\"idx_to_action:\", idx_to_action)\n",
        "    \n",
        "    return state_to_idx, idx_to_state, action_to_idx, idx_to_action\n",
        "\n",
        "def create_feature_matrix(idx_to_state):\n",
        "    \"\"\"\n",
        "    Create task-discriminative features for kitchen environment.\n",
        "    State vector layout (41 dims):\n",
        "    - Indices 0-35: Object locations (9 objects × 4 locations)\n",
        "    - Indices 36-39: Cut status (tomato, onion, mushroom, lettuce)\n",
        "    - Index 40: Stove on/off\n",
        "    \"\"\"\n",
        "    features = []\n",
        "    \n",
        "    for idx in range(len(idx_to_state)):\n",
        "        state = np.array(idx_to_state[idx])\n",
        "        feat = []\n",
        "        \n",
        "        # Features 0-3: Items at each location\n",
        "        for loc in range(4):\n",
        "            feat.append(sum(state[item*4 + loc] for item in range(9)))\n",
        "        \n",
        "        feat.append(np.sum(state[36:40]))  # Feature 4: total cut items\n",
        "        feat.append(state[40])              # Feature 5: stove on\n",
        "        \n",
        "        # Discriminative features for different cooking tasks\n",
        "        pot_at_C, pan_at_C = state[2], state[6]\n",
        "        feat.append(pot_at_C)               # Feature 6: Pot at stove C\n",
        "        feat.append(pan_at_C)               # Feature 7: Pan at stove C\n",
        "        feat.append(state[18])              # Feature 8: Egg at stove C\n",
        "        \n",
        "        tomato_cut, tomato_at_C = state[36], state[14]\n",
        "        feat.append(tomato_cut * tomato_at_C * pot_at_C)  # Feature 9: Tomato cooking\n",
        "        \n",
        "        feat.append(state[22] * pan_at_C)   # Feature 10: Meat cooking\n",
        "        feat.append(state[37])              # Feature 11: Lettuce cut\n",
        "        feat.append(state[38])              # Feature 12: Onion cut\n",
        "        feat.append(state[25] + state[29])  # Feature 13: Vegetables at cutting board\n",
        "        feat.append(state[13])              # Feature 14: Tomato at cutting board\n",
        "        feat.append(state[40] * (pot_at_C + pan_at_C))  # Feature 15: Cooking active\n",
        "        feat.append(state[11])              # Feature 16: Plate at serving location\n",
        "        # Feature 17: Cooked food ready to serve\n",
        "        feat.append((1 - state[40]) * (pot_at_C + pan_at_C) * (state[18] + tomato_at_C + state[22]))\n",
        "        \n",
        "        features.append(feat)\n",
        "    \n",
        "    return np.array(features)\n",
        "\n",
        "def max_entropy_inverse_rl_kitchen(demonstrations, state_to_idx, action_to_idx, feature_matrix,\n",
        "                                   temperature=2.0, gamma=0.9, n_iterations=100, learning_rate=0.05):\n",
        "    \"\"\"Maximum Entropy Inverse RL for sequential kitchen tasks\"\"\"\n",
        "    n_states, n_actions, n_features = len(state_to_idx), len(action_to_idx), feature_matrix.shape[1]\n",
        "    reward_weights = np.zeros(n_features)\n",
        "    \n",
        "    # Compute empirical feature expectations from expert demonstrations\n",
        "    empirical_feature_expectations = np.zeros(n_features)\n",
        "    for trajectory in demonstrations:\n",
        "        discounted_visitation = np.zeros(n_states)\n",
        "        for t, (state, action) in enumerate(trajectory):\n",
        "            discounted_visitation[state_to_idx[state]] += (gamma ** t)\n",
        "        empirical_feature_expectations += discounted_visitation @ feature_matrix\n",
        "    empirical_feature_expectations /= len(demonstrations)\n",
        "    \n",
        "    # Build transition model from demonstrations\n",
        "    transition_model = {}  # (s_idx, a_idx) -> s_next_idx\n",
        "    state_action_pairs = set()\n",
        "    \n",
        "    for trajectory in demonstrations:\n",
        "        for i in range(len(trajectory)):\n",
        "            state, action = trajectory[i]\n",
        "            s_idx, a_idx = state_to_idx[state], action_to_idx[action]\n",
        "            state_action_pairs.add((s_idx, a_idx))\n",
        "            if i < len(trajectory) - 1:\n",
        "                transition_model[(s_idx, a_idx)] = state_to_idx[trajectory[i + 1][0]]\n",
        "    \n",
        "    print(f\"\\nFound {len(state_action_pairs)} unique (state, action) pairs\")\n",
        "    print(f\"Empirical feature expectation norm: {np.linalg.norm(empirical_feature_expectations):.4f}\")\n",
        "    \n",
        "    # Gradient descent with adaptive learning rate\n",
        "    best_diff = float('inf')\n",
        "    patience = 0\n",
        "    best_weights = reward_weights.copy()\n",
        "    momentum = np.zeros(n_features)\n",
        "    momentum_beta = 0.9\n",
        "    \n",
        "    for iteration in range(n_iterations):\n",
        "        current_lr = learning_rate * (0.95 ** (patience // 5))\n",
        "        rewards = feature_matrix @ reward_weights\n",
        "        \n",
        "        # Backward value iteration\n",
        "        q_values = {}  # (s_idx, a_idx) -> Q-value\n",
        "        values = rewards.copy()\n",
        "        \n",
        "        for _ in range(30):\n",
        "            new_values = rewards.copy()\n",
        "            \n",
        "            # Update Q-values\n",
        "            for (s_idx, a_idx) in state_action_pairs:\n",
        "                if (s_idx, a_idx) in transition_model:\n",
        "                    q_values[(s_idx, a_idx)] = rewards[s_idx] + gamma * values[transition_model[(s_idx, a_idx)]]\n",
        "                else:\n",
        "                    q_values[(s_idx, a_idx)] = rewards[s_idx]  # Terminal action\n",
        "            \n",
        "            # Update values using soft-max\n",
        "            for s_idx in range(n_states):\n",
        "                available_qs = [q_values[(s_idx, a_idx)] for a_idx in range(n_actions) if (s_idx, a_idx) in q_values]\n",
        "                if available_qs:\n",
        "                    available_qs = np.array(available_qs)\n",
        "                    max_q = np.max(available_qs)\n",
        "                    new_values[s_idx] = max_q + temperature * np.log(np.sum(np.exp((available_qs - max_q) / temperature)))\n",
        "            \n",
        "            if np.max(np.abs(new_values - values)) < 1e-6:\n",
        "                break\n",
        "            values = new_values\n",
        "        \n",
        "        # Compute soft policy from Q-values\n",
        "        policy = {}\n",
        "        for s_idx in range(n_states):\n",
        "            state_qs = [(a_idx, q_values[(s_idx, a_idx)]) for a_idx in range(n_actions) if (s_idx, a_idx) in q_values]\n",
        "            if state_qs:\n",
        "                state_actions, state_q_vals = zip(*state_qs)\n",
        "                state_q_vals = np.array(state_q_vals)\n",
        "                max_q = np.max(state_q_vals)\n",
        "                probs = np.exp((state_q_vals - max_q) / temperature)\n",
        "                probs /= np.sum(probs)\n",
        "                for a_idx, prob in zip(state_actions, probs):\n",
        "                    policy[(s_idx, a_idx)] = prob\n",
        "        \n",
        "        # Monte Carlo estimation of expected feature counts\n",
        "        expected_feature_counts = np.zeros(n_features)\n",
        "        for _ in range(100):  # Sample 100 trajectories\n",
        "            s_idx = state_to_idx[demonstrations[np.random.randint(len(demonstrations))][0][0]]\n",
        "            traj_features = np.zeros(n_features)\n",
        "            \n",
        "            for t in range(20):  # Max trajectory length\n",
        "                traj_features += (gamma ** t) * feature_matrix[s_idx]\n",
        "                \n",
        "                # Sample action according to policy\n",
        "                available = [(a_idx, policy[(s_idx, a_idx)]) for a_idx in range(n_actions) if (s_idx, a_idx) in policy]\n",
        "                if not available:\n",
        "                    break\n",
        "                \n",
        "                actions, probs = zip(*available)\n",
        "                probs = np.array(probs) / np.sum(probs)\n",
        "                a_idx = np.random.choice(actions, p=probs)\n",
        "                \n",
        "                if (s_idx, a_idx) not in transition_model:\n",
        "                    break\n",
        "                s_idx = transition_model[(s_idx, a_idx)]\n",
        "            \n",
        "            expected_feature_counts += traj_features\n",
        "        expected_feature_counts /= 100\n",
        "        \n",
        "        # Gradient update with momentum\n",
        "        gradient = empirical_feature_expectations - expected_feature_counts\n",
        "        gradient -= 0.01 * reward_weights  # L2 regularization\n",
        "        grad_norm = np.linalg.norm(gradient)\n",
        "        \n",
        "        momentum = momentum_beta * momentum + (1 - momentum_beta) * gradient\n",
        "        reward_weights += current_lr * momentum\n",
        "        \n",
        "        # Track best solution\n",
        "        if grad_norm < best_diff:\n",
        "            best_diff = grad_norm\n",
        "            best_weights = reward_weights.copy()\n",
        "            patience = 0\n",
        "        else:\n",
        "            patience += 1\n",
        "        \n",
        "        if iteration % 10 == 0:\n",
        "            print(f\"Iteration {iteration}: Gradient norm = {grad_norm:.6f}, \"\n",
        "                  f\"Reward range = [{np.min(reward_weights):.2f}, {np.max(reward_weights):.2f}], \"\n",
        "                  f\"Best = {best_diff:.6f}, LR = {current_lr:.4f}\")\n",
        "        \n",
        "        if grad_norm < 0.01 or patience > 20:\n",
        "            print(f\"Converged at iteration {iteration}\")\n",
        "            break\n",
        "    \n",
        "    print(f\"\\nFinal best gradient norm: {best_diff:.6f}\")\n",
        "    recovered_rewards = feature_matrix @ best_weights\n",
        "    return best_weights, recovered_rewards\n",
        "\n",
        "def predict_action(state, reward_weights, feature_matrix, state_to_idx, action_to_idx,\n",
        "                  idx_to_action, transition_model, temperature=2.0, gamma=0.9):\n",
        "    \"\"\"Predict most likely action from a given state\"\"\"\n",
        "    if state not in state_to_idx:\n",
        "        print(f\"Warning: State not seen during training\")\n",
        "        return None, {}\n",
        "    \n",
        "    s_idx = state_to_idx[state]\n",
        "    rewards = feature_matrix @ reward_weights\n",
        "    \n",
        "    # Compute Q-values for all valid actions\n",
        "    valid_actions = []\n",
        "    q_values_list = []\n",
        "    \n",
        "    for a_idx in range(len(action_to_idx)):\n",
        "        if (s_idx, a_idx) in transition_model:\n",
        "            s_next_idx = transition_model[(s_idx, a_idx)]\n",
        "            q_value = rewards[s_idx] + gamma * rewards[s_next_idx]\n",
        "            valid_actions.append(a_idx)\n",
        "            q_values_list.append(q_value)\n",
        "    \n",
        "    if not valid_actions:\n",
        "        print(f\"Warning: No valid actions from this state!\")\n",
        "        return None, {}\n",
        "    \n",
        "    # Compute softmax policy\n",
        "    q_values_array = np.array(q_values_list)\n",
        "    max_q = np.max(q_values_array)\n",
        "    action_probs_array = np.exp((q_values_array - max_q) / temperature)\n",
        "    action_probs_array /= np.sum(action_probs_array)\n",
        "    \n",
        "    # Create action probability dictionary\n",
        "    action_probs = {idx_to_action[a_idx]: prob for a_idx, prob in zip(valid_actions, action_probs_array)}\n",
        "    \n",
        "    # Get most likely action\n",
        "    best_action_idx = valid_actions[np.argmax(action_probs_array)]\n",
        "    predicted_action = idx_to_action[best_action_idx]\n",
        "    \n",
        "    return predicted_action, action_probs\n",
        "\n",
        "# # Main execution\n",
        "# if __name__ == \"__main__\":\n",
        "#     # Load demonstrations\n",
        "#     demonstrations, unique_actions = load_demonstrations('demonstrations_with_states.txt')\n",
        "#     print(f\"Loaded {len(demonstrations)} demonstrations\")\n",
        "#     print(f\"Number of unique actions: {len(unique_actions)}\")\n",
        "#     print(f\"Actions: {unique_actions[:5]}...\")\n",
        "    \n",
        "#     # Create mappings\n",
        "#     state_to_idx, idx_to_state, action_to_idx, idx_to_action = create_state_action_mappings(demonstrations, unique_actions)\n",
        "#     # print(f\"Number of unique states: {len(state_to_idx)}\")\n",
        "    \n",
        "#     # Build transition model\n",
        "#     transition_model = {}\n",
        "#     for trajectory in demonstrations:\n",
        "#         for i in range(len(trajectory)):\n",
        "#             state, action = trajectory[i]\n",
        "#             s_idx, a_idx = state_to_idx[state], action_to_idx[action]\n",
        "#             if i < len(trajectory) - 1: transition_model[(s_idx, a_idx)] = state_to_idx[trajectory[i + 1][0]]\n",
        "    \n",
        "#     # Create feature matrix\n",
        "#     feature_matrix = create_feature_matrix(idx_to_state)\n",
        "#     print(f\"Feature matrix shape: {feature_matrix.shape}\")\n",
        "    \n",
        "#     # Run IRL\n",
        "#     print(\"\\nRunning Maximum Entropy IRL...\")\n",
        "#     reward_weights, recovered_rewards = max_entropy_inverse_rl_kitchen(\n",
        "#         demonstrations=demonstrations,\n",
        "#         state_to_idx=state_to_idx,\n",
        "#         action_to_idx=action_to_idx,\n",
        "#         feature_matrix=feature_matrix,\n",
        "#         temperature=1.0,\n",
        "#         gamma=0.95,\n",
        "#         n_iterations=100,\n",
        "#         learning_rate=0.01\n",
        "#     )\n",
        "    \n",
        "#     print(\"\\n=== Results ===\")\n",
        "#     print(f\"Learned reward weights shape: {reward_weights.shape}\")\n",
        "#     print(f\"Reward weight statistics:\")\n",
        "#     print(f\"  Mean: {np.mean(reward_weights):.4f}\")\n",
        "#     print(f\"  Std: {np.std(reward_weights):.4f}\")\n",
        "#     print(f\"  Min: {np.min(reward_weights):.4f}\")\n",
        "#     print(f\"  Max: {np.max(reward_weights):.4f}\")\n",
        "    \n",
        "#     print(f\"\\nRecovered rewards statistics:\")\n",
        "#     print(f\"  Mean: {np.mean(recovered_rewards):.4f}\")\n",
        "#     print(f\"  Std: {np.std(recovered_rewards):.4f}\")\n",
        "#     print(f\"  Min: {np.min(recovered_rewards):.4f}\")\n",
        "#     print(f\"  Max: {np.max(recovered_rewards):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Adaptive IRL - Train on One Recipe, Test on Another"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function to test a single demonstration\n",
        "def test_demonstration(test_demo, reward_weights, feature_matrix, state_to_idx, action_to_idx, idx_to_action, transition_model):\n",
        "    \"\"\"Test predictions on a single demonstration and return results\"\"\"\n",
        "    total, correct = 0, 0\n",
        "    step_results = []\n",
        "\n",
        "    \n",
        "    for i in range(len(test_demo) - 1):  # Exclude last state (stop action)\n",
        "        state, expert_action = test_demo[i]\n",
        "        # Predict action\n",
        "        predicted_action, _ = predict_action(state, reward_weights, feature_matrix, state_to_idx, action_to_idx, idx_to_action, transition_model)\n",
        "        if predicted_action is None:\n",
        "            predicted_action = \"<UNSEEN_STATE>\"\n",
        "            is_correct = False\n",
        "        else: is_correct = (predicted_action == expert_action)\n",
        "        # is_correct = (predicted_action == expert_action)\n",
        "        status = \"✓\" if is_correct else \"✗\"\n",
        "        \n",
        "        print(f\"  Step {i+1}: Expert={expert_action:30s} Predicted={predicted_action:30s} {status}\")\n",
        "        \n",
        "        step_results.append({'step': i+1, 'correct': is_correct})\n",
        "        total += 1\n",
        "        if is_correct: correct += 1\n",
        "    \n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "    return accuracy, correct, total, step_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate separate training and test datasets\n",
        "print(\"GENERATING TRAINING DATA (Recipe 1: Tomato first)\")\n",
        "gen_train = RecipeGenerator()\n",
        "for i in range(3):  # num demonstrations of recipe 1\n",
        "    actions = gen_train.generate_tomato_onion_soup_1()\n",
        "    gen_train._record_trajectory(actions)\n",
        "# Save training data\n",
        "with open(\"train_demos.txt\", \"w\") as f:\n",
        "    for demo in gen_train.demos: f.write(json.dumps(demo) + \"\\n\")\n",
        "\n",
        "\n",
        "print(\"GENERATING TEST DATA (Recipe 2: Onion first)\")\n",
        "gen_test = RecipeGenerator()\n",
        "for i in range(2):  # num demonstrations of recipe 2\n",
        "    actions = gen_test.generate_tomato_onion_soup_2()\n",
        "    gen_test._record_trajectory(actions)\n",
        "print(\"GENERATING TEST DATA (Recipe 3: Mushroom)\")\n",
        "for i in range(2):  # num demonstrations of recipe 2\n",
        "    actions = gen_test.generate_mushroom_soup()\n",
        "    gen_test._record_trajectory(actions)\n",
        "print(\"GENERATING TEST DATA (Recipe 4: Just tomato)\")\n",
        "for i in range(2):  # num demonstrations of recipe 2\n",
        "    actions = gen_test.generate_tomato_soup()\n",
        "    gen_test._record_trajectory(actions)\n",
        "# Save test data\n",
        "with open(\"test_demos.txt\", \"w\") as f:\n",
        "    for demo in gen_test.demos: f.write(json.dumps(demo) + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# INITIAL TRAINING on Recipe 1 only\n",
        "print(\"INITIAL TRAINING: Learning from 1 recipe only\")\n",
        "# Load training demonstrations\n",
        "train_demonstrations, train_actions = load_demonstrations('train_demos.txt')\n",
        "print(f\"Loaded {len(train_demonstrations)} training demonstrations\\n\")\n",
        "\n",
        "# Create mappings\n",
        "state_to_idx, idx_to_state, action_to_idx, idx_to_action = create_state_action_mappings(train_demonstrations, train_actions)\n",
        "\n",
        "# Build transition model\n",
        "transition_model = {}\n",
        "for trajectory in train_demonstrations:\n",
        "    for i in range(len(trajectory)):\n",
        "        state, action = trajectory[i]\n",
        "        s_idx, a_idx = state_to_idx[state], action_to_idx[action]\n",
        "        if i < len(trajectory) - 1: transition_model[(s_idx, a_idx)] = state_to_idx[trajectory[i + 1][0]]\n",
        "\n",
        "# Create feature matrix\n",
        "feature_matrix = create_feature_matrix(idx_to_state)\n",
        "print(f\"\\nFeature matrix shape: {feature_matrix.shape}\")\n",
        "\n",
        "# Run IRL\n",
        "print(\"\\nRunning IRL...\")\n",
        "reward_weights, recovered_rewards = max_entropy_inverse_rl_kitchen(\n",
        "    demonstrations=train_demonstrations,\n",
        "    state_to_idx=state_to_idx,\n",
        "    action_to_idx=action_to_idx,\n",
        "    feature_matrix=feature_matrix,\n",
        "    temperature=1.0,\n",
        "    gamma=0.95,\n",
        "    n_iterations=100,\n",
        "    learning_rate=0.01\n",
        ")\n",
        "print(f\"\\nInitial training complete!\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ADAPTIVE IRL: Test on Recipe 2, then retrain\n",
        "print(\"ADAPTIVE IRL LOOP\")\n",
        "# Load test demonstrations\n",
        "test_demonstrations, _ = load_demonstrations('test_demos.txt')\n",
        "# Keep track of all training data\n",
        "all_training_data = train_demonstrations.copy()\n",
        "test_history = []\n",
        "\n",
        "# Process each test demonstration\n",
        "for test_num, test_demo in enumerate(test_demonstrations, 1):\n",
        "    print(f\"TEST {test_num}/{len(test_demonstrations)}: Evaluating Unseen Recipe Demo\")\n",
        "\n",
        "    # Test with current model\n",
        "    accuracy, correct, total, step_results = test_demonstration(\n",
        "        test_demo, reward_weights, feature_matrix, state_to_idx,\n",
        "        action_to_idx, idx_to_action, transition_model)\n",
        "    print(f\"\\nTest Accuracy: {accuracy:.1%} ({correct}/{total} correct predictions)\")\n",
        "    # Record results\n",
        "    test_history.append({\n",
        "        'test_num': test_num,   'accuracy': accuracy,   'correct': correct,\n",
        "        'total': total,         'training_size_before': len(all_training_data)})    \n",
        "    # Add test demo to training data\n",
        "    print(f\"\\n>>> Adding test demonstration to training set...\")\n",
        "    all_training_data.append(test_demo)\n",
        "    print(f\">>> Training set size: {len(train_demonstrations)} → {len(all_training_data)}\")\n",
        "\n",
        "\n",
        "    # Recreate mappings (may have new states/actions)\n",
        "    unique_actions_new = set()\n",
        "    for traj in all_training_data: unique_actions_new.update(action for _, action in traj)\n",
        "    unique_actions_new = sorted(list(unique_actions_new))\n",
        "    state_to_idx, idx_to_state, action_to_idx, idx_to_action = create_state_action_mappings(all_training_data, unique_actions_new)\n",
        "    # Rebuild transition model\n",
        "    transition_model = {}\n",
        "    for trajectory in all_training_data:\n",
        "        for i in range(len(trajectory)):\n",
        "            state, action = trajectory[i]\n",
        "            s_idx, a_idx = state_to_idx[state], action_to_idx[action]\n",
        "            if i < len(trajectory) - 1: transition_model[(s_idx, a_idx)] = state_to_idx[trajectory[i + 1][0]]\n",
        "    # Rebuild feature matrix\n",
        "    feature_matrix = create_feature_matrix(idx_to_state)\n",
        "    print(f\"States: {len(state_to_idx)}, Actions: {len(action_to_idx)}, Features: {feature_matrix.shape[1]}\")\n",
        "    \n",
        "    # Run IRL\n",
        "    reward_weights, recovered_rewards = max_entropy_inverse_rl_kitchen(\n",
        "        demonstrations=all_training_data,   state_to_idx=state_to_idx,      action_to_idx=action_to_idx,\n",
        "        feature_matrix=feature_matrix,      temperature=1.0,gamma=0.95,     n_iterations=100,   learning_rate=0.01)\n",
        "    print(f\"\\nRetraining complete. Model updated with {len(all_training_data)} total demonstrations.\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 2: Test first 6 steps of first 3 trajectories\n",
        "l = len(all_training_data)\n",
        "for i in range(l):\n",
        "    print(i)\n",
        "    le = len(all_training_data[i])\n",
        "    for j in range(le):\n",
        "        state = all_training_data[i][j][0]\n",
        "        action, probs = predict_action(state, reward_weights, feature_matrix,\n",
        "                                      state_to_idx, action_to_idx, idx_to_action, transition_model)\n",
        "        print(f\"Actual: {all_training_data[i][j][1]} \\n Predicted: {action} \\n\")\n",
        "    print('\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary of Adaptive Learning\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ADAPTIVE LEARNING SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Initial training: {len(train_demonstrations)} demos (Recipe 1 only)\")\n",
        "print(f\"Final training: {len(all_training_data)} demos (Recipe 1 + Recipe 2)\")\n",
        "print(f\"Tests conducted: {len(test_history)}\\n\")\n",
        "\n",
        "print(\"Test-by-test results:\")\n",
        "print(f\"{'Test':<8} {'Train Size':<12} {'Accuracy':<12} {'Correct/Total':<15}\")\n",
        "print(\"-\" * 70)\n",
        "for result in test_history:\n",
        "    print(f\"{result['test_num']:<8} {result['training_size_before']:<12} \"\n",
        "          f\"{result['accuracy']:>6.1%}{'':5} {result['correct']}/{result['total']}\")\n",
        "\n",
        "if test_history:\n",
        "    avg_accuracy = np.mean([r['accuracy'] for r in test_history])\n",
        "    print(f\"\\nAverage accuracy across tests: {avg_accuracy:.1%}\")\n",
        "    \n",
        "    if len(test_history) > 1:\n",
        "        improvement = test_history[-1]['accuracy'] - test_history[0]['accuracy']\n",
        "        print(f\"Accuracy improvement (Test 1 → Test {len(test_history)}): {improvement:+.1%}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Key Insight: Model learned to handle BOTH recipe orderings!\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Visualize learning progress\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if test_history:\n",
        "    test_nums = [r['test_num'] for r in test_history]\n",
        "    accuracies = [r['accuracy'] for r in test_history]\n",
        "    train_sizes = [r['training_size_before'] + r['test_num'] for r in test_history]\n",
        "    \n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Plot 1: Accuracy over tests\n",
        "    ax1.plot(test_nums, accuracies, marker='o', linewidth=2, markersize=10, color='#2E86AB')\n",
        "    ax1.set_xlabel('Test Number', fontsize=12, fontweight='bold')\n",
        "    ax1.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
        "    ax1.set_title('Adaptive IRL: Accuracy on Unseen Recipe', fontsize=14, fontweight='bold')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.set_ylim(0, 1.05)\n",
        "    for x, y in zip(test_nums, accuracies):\n",
        "        ax1.text(x, y + 0.03, f'{y:.0%}', ha='center', fontsize=10, fontweight='bold')\n",
        "    \n",
        "    # Plot 2: Training set growth\n",
        "    ax2.bar(test_nums, train_sizes, color='#A23B72', alpha=0.7, edgecolor='black')\n",
        "    ax2.axhline(y=len(train_demonstrations), color='red', linestyle='--', label='Initial training size', linewidth=2)\n",
        "    ax2.set_xlabel('Test Number', fontsize=12, fontweight='bold')\n",
        "    ax2.set_ylabel('Training Set Size', fontsize=12, fontweight='bold')\n",
        "    ax2.set_title('Training Data Growth (Recipe 1 → Recipe 1+2)', fontsize=14, fontweight='bold')\n",
        "    ax2.grid(True, alpha=0.3, axis='y')\n",
        "    ax2.legend()\n",
        "    for x, y in zip(test_nums, train_sizes):\n",
        "        ax2.text(x, y + 0.1, f'{y}', ha='center', fontsize=10, fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\nVisualization complete!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "xN5BqgVn93Bk"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.12.3)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
